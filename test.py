# -*- coding: utf-8 -*-
import torch
import logging
from unsloth import FastLanguageModel
from transformers import TextStreamer, AutoTokenizer
from typing import List

# --- é…ç½®åŒºåŸŸ ---

# è¯·å°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºæ‚¨å¾®è°ƒå¹¶åˆå¹¶åçš„æ¨¡å‹æ‰€åœ¨çš„ç›®å½•
# è¿™ä¸ªç›®å½•åº”è¯¥åŒ…å« model.safetensors, config.json, tokenizer.json ç­‰æ–‡ä»¶
MODEL_PATH = "/mnt/wangjingxiong/think_on_graph/mpo_models/cand_pn_only_pos_shortest_paths/hf_dataset/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec_ep1_loss-mpo_b0.1_sft0.05_lr1e-5_lora-r8-a16_bf16/merged_model" 

# åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨çš„ç²¾åº¦ã€‚åº”ä¸æ‚¨è®­ç»ƒæ—¶ä½¿ç”¨çš„ç²¾åº¦ä¸€è‡´ã€‚
# æ‚¨çš„è®­ç»ƒè„šæœ¬ä¸­ BF16=True, æ‰€ä»¥è¿™é‡Œä¹Ÿä½¿ç”¨ "bf16"
DTYPE = "bf16" 

# åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨çš„é‡åŒ–æ–¹å¼ã€‚æ‚¨çš„è®­ç»ƒè„šæœ¬ä¸­ LOAD_IN_8BIT=Trueã€‚
# å¦‚æœæ˜¯4-bitï¼Œè¯·è®¾ç½®ä¸º "4bit"ã€‚å¦‚æœæ˜¯8-bitï¼Œåˆ™ä¸º "8bit"ã€‚å¦‚æœæ— é‡åŒ–ï¼Œåˆ™ä¸º Noneã€‚
LOAD_IN_QUANTIZATION = "4bit" # "8bit", "4bit", or None

# -----------------

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_model_and_tokenizer(model_path: str, dtype: str, load_in_quant: str):
    """
    ä½¿ç”¨ unsloth é«˜æ•ˆåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    logging.info(f"[*] æ­£åœ¨ä» '{model_path}' åŠ è½½æ¨¡å‹...")
    
    # æ ¹æ®é‡åŒ–è®¾ç½®ç¡®å®šåŠ è½½å‚æ•°
    load_in_4bit = (load_in_quant == "4bit")
    load_in_8bit = (load_in_quant == "8bit")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=1024, # åº”ä¸è®­ç»ƒæ—¶è®¾ç½®çš„ max_length ä¿æŒä¸€è‡´
        dtype=getattr(torch, dtype, None),
        load_in_4bit=load_in_4bit,
        load_in_8bit=load_in_8bit,
    )
    
    # ä¸ºæ¨¡å‹å¯ç”¨ PEFT (å¦‚æœæ¨¡å‹æ˜¯ LoRA åˆå¹¶çš„ï¼Œunsloth ä¼šè‡ªåŠ¨å¤„ç†)
    # å¦‚æœæ‚¨åŠ è½½çš„æ˜¯åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨ï¼Œåˆ™éœ€è¦ä½¿ç”¨ PeftModel.from_pretrained
    # ä½†æ‚¨çš„è·¯å¾„æŒ‡å‘ä¸€ä¸ª merged_modelï¼Œæ‰€ä»¥è¿™ç§ç›´æ¥åŠ è½½çš„æ–¹å¼æ˜¯æ­£ç¡®çš„ã€‚
    logging.info("[*] æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸã€‚")

    # é…ç½®åˆ†è¯å™¨çš„å¡«å……(padding)åŠŸèƒ½ï¼Œè¿™å¯¹äºæ‰¹å¤„ç†è‡³å…³é‡è¦
    if tokenizer.pad_token is None:
        # å¦‚æœæ¨¡å‹æ²¡æœ‰é¢„è®¾çš„ pad_tokenï¼Œé€šå¸¸ä½¿ç”¨ eos_token ä½œä¸ºæ›¿ä»£
        tokenizer.pad_token = tokenizer.eos_token 
        logging.info("åˆ†è¯å™¨çš„ pad_token æœªè®¾ç½®ï¼Œå·²å°†å…¶è®¾ç½®ä¸º eos_tokenã€‚")
    
    return model, tokenizer

def test_batch_generation_with_formatted_prompts(model, tokenizer):
    """
    æµ‹è¯•å¹¶æ¼”ç¤ºæ¨¡å‹å¤„ç†é¢„æ ¼å¼åŒ–promptçš„æ‰¹å¤„ç†èƒ½åŠ›ã€‚
    """
    print("\n--- å¼€å§‹æ‰¹é‡ç”Ÿæˆæµ‹è¯• (ä½¿ç”¨é¢„æ ¼å¼åŒ–çš„æŒ‡ä»¤) ---")
    
    # 1. å®šä¹‰ä¸€ä¸ªåŒ…å«å¤šä¸ªé¢„æ ¼å¼åŒ–æŒ‡ä»¤çš„è¾“å…¥æ‰¹æ¬¡
    #    è¿™æ¨¡æ‹Ÿäº†æ‚¨åœ¨å…³ç³»é€‰æ‹©ä»»åŠ¡ä¸­ä¼šé‡åˆ°çš„æƒ…å†µ
    pre_formatted_prompts = [
f"""*Role:** KG Strategist
**Objective:** Identify paths to answer: "what is the name of justin bieber brother"
**Current Entity:** "Justin Bieber"
**Task:** From the 'Available Relations' listed below, select **up to 5** distinct relations.
You MUST choose relations that are MOST LIKELY to lead to relevant information for the Objective.
**Available Relations for "Justin Bieber":**
```
base.popstra.celebrity.breakup-base.popstra.breakup.participant
base.popstra.celebrity.friendship-base.popstra.friendship.participant
celebrities.celebrity.celebrity_friends-celebrities.friendship.friend
people.person.sibling_s-people.sibling_relationship.sibling
```
**Output Requirements:**
* Respond ONLY with the selected relations. NO other text, explanations, or comments.
* Each selected relation MUST be an **exact, verbatim copy** of a complete line from the 'Available Relations' list above.
* Output EACH selected relation on a **new line**.
**Example of Correct Output Format (if REL_A and REL_B were selected):**
[REL_A] chosen.relation.example_one
[REL_B] another.chosen.relation
**Your Selection:""",
f"""**Role:** KG Strategist
**Objective:** Identify paths to answer: "what character did natalie portman play in star wars"
**Current Entity:** "Natalie Portman"
**Task:** From the 'Available Relations' listed below, select **up to 5** distinct relations.
You MUST choose relations that are MOST LIKELY to lead to relevant information for the Objective.
**Available Relations for "Natalie Portman":**
```
film.actor.film-film.performance.character
film.actor.film-film.performance.special_performance_type
tv.tv_actor.guest_roles-tv.tv_guest_role.episodes_appeared_in
tv.tv_actor.guest_roles-tv.tv_guest_role.special_performance_type
```
**Output Requirements:**
* Respond ONLY with the selected relations. NO other text, explanations, or comments.
* Each selected relation MUST be an **exact, verbatim copy** of a complete line from the 'Available Relations' list above.
* Output EACH selected relation on a **new line**.
**Example of Correct Output Format (if REL_A and REL_B were selected):**
[REL_A] chosen.relation.example_one
[REL_B] another.chosen.relation
**Your Selection:""",
f"""**Role:** KG Strategist
**Objective:** Identify paths to answer: "what country is the grand bahama island in"
**Current Entity:** "Grand Bahama"
**Task:** From the 'Available Relations' listed below, select **up to 5** distinct relations.
You MUST choose relations that are MOST LIKELY to lead to relevant information for the Objective.
**Available Relations for "Grand Bahama":**
```
common.topic.webpage-common.webpage.in_index
location.location.containedby
location.location.nearby_airports
location.location.people_born_here
```
**Output Requirements:**
* Respond ONLY with the selected relations. NO other text, explanations, or comments.
* Each selected relation MUST be an **exact, verbatim copy** of a complete line from the 'Available Relations' list above.
* Output EACH selected relation on a **new line**.
**Example of Correct Output Format (if REL_A and REL_B were selected):**
[REL_A] chosen.relation.example_one
[REL_B] another.chosen.relation
**Your Selection:"""
    ]
    
    logging.info(f"[*] å‡†å¤‡å¤„ç† {len(pre_formatted_prompts)} ä¸ªé¢„æ ¼å¼åŒ–promptçš„æ‰¹æ¬¡...")

    # 2. å¯¹æ•´ä¸ªæ‰¹æ¬¡è¿›è¡Œåˆ†è¯
    #    padding=True ä¼šå°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰å¥å­å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦
    #    return_tensors="pt" è¿”å› PyTorch å¼ é‡
    inputs = tokenizer(pre_formatted_prompts, padding=True, return_tensors="pt").to("cuda")

    # 3. å®šä¹‰ç”Ÿæˆå‚æ•°
    generation_params = {
        "max_new_tokens": 128,  # å¯¹äºå…³ç³»é€‰æ‹©ä»»åŠ¡ï¼Œå›å¤é€šå¸¸è¾ƒçŸ­
        "do_sample": False,     # å¯¹äºæŒ‡ä»¤ä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ç¡®å®šæ€§è§£ç 
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }

    # 4. ä½¿ç”¨å•æ¬¡è°ƒç”¨å¤„ç†æ•´ä¸ªæ‰¹æ¬¡
    logging.info("[*] è°ƒç”¨ model.generate å¤„ç†æ•´ä¸ªæ‰¹æ¬¡... ğŸ¤– æ¨¡å‹æ­£åœ¨ç”Ÿæˆ...")
    outputs = model.generate(**inputs, **generation_params)
    
    # 5. å¯¹æ‰¹å¤„ç†ç»“æœè¿›è¡Œè§£ç 
    #    skip_special_tokens=True ä¼šåœ¨è§£ç æ—¶ç§»é™¤ç‰¹æ®Šçš„tokenï¼ˆå¦‚ padding, eosï¼‰
    batch_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    
    # 6. æ‰“å°æ‰¹å¤„ç†ç»“æœ
    #    æ³¨æ„ï¼šç”Ÿæˆçš„æ–‡æœ¬ä¼šåŒ…å«åŸå§‹çš„promptéƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­æå–å‡ºæ¨¡å‹çœŸæ­£æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    print("\n--- æ‰¹é‡ç”Ÿæˆç»“æœ ---")
    for i, (original_prompt, full_response) in enumerate(zip(pre_formatted_prompts, batch_responses)):
        # æå–æ¨¡å‹æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_text = full_response[len(original_prompt):].strip()
        
        print("-" * 50)
        print(f"ğŸ‘¤ æ ¼å¼åŒ–è¾“å…¥ {i+1}:\n{original_prompt}\n")
        print(f"ğŸ¤– æ¨¡å‹å›å¤ (ä»…æ–°ç”Ÿæˆéƒ¨åˆ†):\n{generated_text}")
    print("-" * 50)


def main():
    """ä¸»å‡½æ•°"""
    try:
        model, tokenizer = load_model_and_tokenizer(MODEL_PATH, DTYPE, LOAD_IN_QUANTIZATION)
        test_batch_generation_with_formatted_prompts(model, tokenizer)
    except FileNotFoundError:
        logging.error(f"[!] æ¨¡å‹è·¯å¾„æœªæ‰¾åˆ°: '{MODEL_PATH}'")
        logging.error("[!] è¯·ç¡®ä¿ MODEL_PATH å˜é‡æŒ‡å‘äº†æ­£ç¡®çš„ä½ç½®ã€‚")
    except Exception as e:
        logging.error(f"\n[!] æ¨ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        logging.error("[!] è¯·æ£€æŸ¥æ‚¨çš„ç¯å¢ƒã€æ¨¡å‹è·¯å¾„å’ŒCUDAè®¾ç½®ã€‚")

if __name__ == "__main__":
    main()
