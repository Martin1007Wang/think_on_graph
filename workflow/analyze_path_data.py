import json
import argparse
import os
import re
from typing import List, Tuple, Dict, Any
from collections import Counter, defaultdict
import logging

# ... (é¡¶éƒ¨çš„å‡½æ•° parse_path_to_segments, build_history_key, print_distribution ä¿æŒä¸å˜) ...
# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Pre-compiled Regex for Performance ---
PATH_STEP_REGEX = re.compile(r"(.+?)\s*-\[\s*(.+?)\s*\]->\s*(.+)")

def parse_path_to_segments(path_str: str) -> List[Tuple[str, str, str]]:
    """å°†è·¯å¾„å­—ç¬¦ä¸²è§£æä¸º (æºå®ä½“, å…³ç³», ç›®æ ‡å®ä½“) çš„æ®µåˆ—è¡¨ã€‚"""
    if not path_str:
        return []
    segments: List[Tuple[str, str, str]] = []
    path_steps = path_str.split(" ; ")
    for step in path_steps:
        match = PATH_STEP_REGEX.match(step)
        if match:
            src, rel, tgt = match.groups()
            segments.append((src.strip(), rel.strip(), tgt.strip()))
    return segments

def build_history_key(history_segments: List[Tuple[str, str, str]]) -> str:
    """æ ¹æ®å†å²æ®µè½æ„å»ºä¸€ä¸ªå”¯ä¸€çš„å­—ç¬¦ä¸²é”®ã€‚"""
    if not history_segments:
        return ""
    return " ; ".join([f"{s}-[{r}]->{t}" for s, r, t in history_segments])

def print_distribution(title: str, counter: Counter, top_n: int = 20):
    """ä»¥å¯è¯»çš„æ ¼å¼æ‰“å°åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ã€‚"""
    print("\n" + "="*80)
    print(f"ğŸ“Š {title}")
    print("="*80)
    
    if not counter:
        print("  (No data to display)")
        return

    total_count = sum(counter.values())
    print(f"{'Value':<20} | {'Count':>10} | {'Percentage':>12} | {'Cumulative %':>15}")
    print("-" * 80)
    
    sorted_items = sorted(counter.items(), key=lambda x: x[1], reverse=True)
    
    cumulative_percentage = 0.0
    for i, (value, count) in enumerate(sorted_items):
        percentage = (count / total_count) * 100
        cumulative_percentage += percentage
        
        # å¯¹äºå…³ç³»é¢‘ç‡ï¼Œvalueå¯èƒ½å¾ˆé•¿ï¼Œéœ€è¦æˆªæ–­
        display_value = str(value)
        if len(display_value) > 18:
            display_value = display_value[:15] + "..."

        print(f"{display_value:<20} | {count:>10} | {percentage:>11.2f}% | {cumulative_percentage:>14.2f}%")
        
        if i >= top_n - 1 and len(sorted_items) > top_n:
            remaining_items = len(sorted_items) - top_n
            print(f"... and {remaining_items} more ...")
            break


def analyze_path_data(input_paths: List[str]):
    """ä¸»åˆ†æå‡½æ•°ï¼Œå¤„ç†æ‰€æœ‰è¾“å…¥æ–‡ä»¶å¹¶èšåˆç»Ÿè®¡æ•°æ®ã€‚"""
    
    # ç»Ÿè®¡æ•°æ®ç»“æ„ä¿æŒä¸å˜
    positive_path_lengths = Counter()
    negative_path_lengths = Counter()
    total_relation_counts = Counter()
    positive_relation_counts = Counter()
    negative_relation_counts = Counter()
    choices_per_state = defaultdict(lambda: {"positive": set(), "negative": set()})
    total_items_processed = 0
    
    logger.info(f"Starting analysis on {len(input_paths)} file(s)...")

    all_data = []
    for file_path in input_paths:
        logger.info(f"Processing file: {file_path}")
        try:
            # ===============================================================
            #  æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œï¼šä½¿ç”¨ json.load() æ¥è¯»å–æ•´ä¸ªæ–‡ä»¶
            # ===============================================================
            with open(file_path, 'r', encoding='utf-8') as f:
                # json.load(f) ä¼šå°†æ•´ä¸ªæ–‡ä»¶å†…å®¹ä½œä¸ºä¸€ä¸ªå•ä¸€çš„JSONå¯¹è±¡/æ•°ç»„è¿›è¡Œè§£æ
                loaded_data = json.load(f)
                
                # ç¡®ä¿åŠ è½½çš„æ•°æ®æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if isinstance(loaded_data, list):
                    all_data.extend(loaded_data)
                else:
                    logger.warning(f"File {file_path} does not contain a JSON array at the top level. Skipping.")
                    continue
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse {file_path} as a single, valid JSON file. Error: {e}")
            continue
        except Exception as e:
            logger.error(f"An unexpected error occurred while loading {file_path}: {e}")
            continue

    logger.info(f"Successfully loaded a total of {len(all_data)} items from all files.")
    
    for item in all_data:
        total_items_processed += 1
        
        # --- åç»­çš„ç»Ÿè®¡é€»è¾‘å®Œå…¨ä¸éœ€è¦æ”¹å˜ ---
        item_states = defaultdict(lambda: {"positive": set(), "negative": set()})

        for path_str in item.get('positive_paths', []) + item.get('shortest_paths', []):
            segments = parse_path_to_segments(path_str)
            positive_path_lengths[len(segments)] += 1
            history = []
            for src, rel, tgt in segments:
                total_relation_counts[rel] += 1
                positive_relation_counts[rel] += 1
                state_key = (build_history_key(history), src)
                item_states[state_key]["positive"].add(rel)
                history.append((src, rel, tgt))
        
        for path_str in item.get('negative_paths', []):
            segments = parse_path_to_segments(path_str)
            negative_path_lengths[len(segments)] += 1
            history = []
            for src, rel, tgt in segments:
                total_relation_counts[rel] += 1
                negative_relation_counts[rel] += 1
                state_key = (build_history_key(history), src)
                item_states[state_key]["negative"].add(rel)
                history.append((src, rel, tgt))
        
        for state_key, relations in item_states.items():
            choices_per_state[state_key]["positive"].update(relations["positive"])
            choices_per_state[state_key]["negative"].update(relations["negative"])

    logger.info("Analysis complete. Aggregating final statistics...")

    # ... (åç»­çš„æ‰“å°æŠ¥å‘Šå’Œä¿å­˜JSONçš„é€»è¾‘ä¿æŒå®Œå…¨ä¸å˜) ...
    num_pos_choices_dist = Counter(len(state["positive"]) for state in choices_per_state.values())
    num_neg_choices_dist = Counter(len(state["negative"]) for state in choices_per_state.values())
    total_choices_dist = Counter(len(state["positive"]) + len(state["negative"]) for state in choices_per_state.values())
    
    # --- æ‰“å°æŠ¥å‘Š ---
    print("\n" + "#"*80)
    print("###               Paths Dataset Statistical Analysis Report               ###")
    print("#"*80)
    print(f"\n- Total Questions/Items Processed: {total_items_processed}")
    print(f"- Total Unique Decision Points (Entity + History): {len(choices_per_state)}")

    print_distribution("Distribution of POSITIVE Choices per Decision Point", num_pos_choices_dist)
    print_distribution("Distribution of NEGATIVE Choices per Decision Point", num_neg_choices_dist)
    print_distribution("Distribution of TOTAL Choices per Decision Point", total_choices_dist)
    
    print_distribution("Distribution of Positive Path Lengths (hops)", positive_path_lengths)
    print_distribution("Distribution of Negative Path Lengths (hops)", negative_path_lengths)
    
    print_distribution("Top 20 Most Frequent Relations (Overall)", total_relation_counts)
    print_distribution("Top 20 Most Frequent Relations in POSITIVE Paths", positive_relation_counts)
    print_distribution("Top 20 Most Frequent Relations in NEGATIVE Paths", negative_relation_counts)

    # --- ä¿å­˜è¯¦ç»†ç»Ÿè®¡æ•°æ®åˆ°JSON ---
    output_stats = {
        "summary": {
            "total_items_processed": total_items_processed,
            "total_unique_decision_points": len(choices_per_state)
        },
        "distributions": {
            "positive_choices_per_state": dict(num_pos_choices_dist),
            "negative_choices_per_state": dict(num_neg_choices_dist),
            "total_choices_per_state": dict(total_choices_dist),
            "positive_path_lengths": dict(positive_path_lengths),
            "negative_path_lengths": dict(negative_path_lengths)
        },
        "frequencies": {
            "top_50_total_relations": dict(total_relation_counts.most_common(50)),
            "top_50_positive_relations": dict(positive_relation_counts.most_common(50)),
            "top_50_negative_relations": dict(negative_relation_counts.most_common(50))
        }
    }
    
    output_filename = "path_data_statistics.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_stats, f, indent=4)
        
    logger.info(f"Detailed statistics saved to '{output_filename}'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze path-based datasets to gather statistics for preference data creation.")
    parser.add_argument('input_paths', nargs='+', help='One or more paths to the input standard JSON or JSONL data files.')
    args = parser.parse_args()
    
    # æ³¨æ„ï¼šè¿™ä¸ªè„šæœ¬ç°åœ¨ä¸»è¦è®¾è®¡ä¸ºå¤„ç†æ ‡å‡†JSONæ•°ç»„æ–‡ä»¶ã€‚
    # å¦‚æœæ‚¨ç¡®å®šæ–‡ä»¶æ˜¯JSON Linesï¼Œè¯·ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çš„è„šæœ¬ç‰ˆæœ¬ã€‚
    analyze_path_data(args.input_paths)