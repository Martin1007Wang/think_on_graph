2025-03-19 13:17:29,155 - ERROR - Training pipeline failed: argument --seed: conflicting option string: --seed
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    parser = HfArgumentParser((ScriptArguments, TrainingConfig))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 142, in __init__
    self._add_dataclass_arguments(dtype)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 282, in _add_dataclass_arguments
    self._parse_dataclass_field(parser, field)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 234, in _parse_dataclass_field
    parser.add_argument(*long_options, *aliases, **kwargs)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1500, in add_argument
    return self._add_action(action)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1885, in _add_action
    self._optionals._add_action(action)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1705, in _add_action
    action = super(_ArgumentGroup, self)._add_action(action)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1514, in _add_action
    self._check_conflict(action)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1654, in _check_conflict
    conflict_handler(action, confl_optionals)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1663, in _handle_conflict_error
    raise ArgumentError(action, message % conflict_string)
argparse.ArgumentError: argument --seed: conflicting option string: --seed
2025-03-19 13:17:29,160 - ERROR - Training pipeline failed: argument --seed: conflicting option string: --seed
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    parser = HfArgumentParser((ScriptArguments, TrainingConfig))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 142, in __init__
    self._add_dataclass_arguments(dtype)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 282, in _add_dataclass_arguments
    self._parse_dataclass_field(parser, field)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/hf_argparser.py", line 234, in _parse_dataclass_field
    parser.add_argument(*long_options, *aliases, **kwargs)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1500, in add_argument
    return self._add_action(action)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1885, in _add_action
    self._optionals._add_action(action)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1705, in _add_action
    action = super(_ArgumentGroup, self)._add_action(action)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1514, in _add_action
    self._check_conflict(action)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1654, in _check_conflict
    conflict_handler(action, confl_optionals)
  File "/anaconda3/envs/GCR/lib/python3.12/argparse.py", line 1663, in _handle_conflict_error
    raise ArgumentError(action, message % conflict_string)
argparse.ArgumentError: argument --seed: conflicting option string: --seed
2025-03-19 13:19:44,770 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:19:44,771 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-19-44_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:19:44,772 - INFO - Starting training pipeline
2025-03-19 13:19:44,772 - ERROR - Training pipeline failed: 'ScriptArguments' object has no attribute 'seed'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 515, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 424, in train
    set_seed(args.seed)
             ^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'seed'
2025-03-19 13:19:44,861 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:19:44,862 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-19-44_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:19:44,863 - INFO - Starting training pipeline
2025-03-19 13:19:44,863 - ERROR - Training pipeline failed: 'ScriptArguments' object has no attribute 'seed'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 515, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 424, in train
    set_seed(args.seed)
             ^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'seed'
2025-03-19 13:20:17,321 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:20:17,322 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-20-17_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:20:17,322 - INFO - Starting training pipeline
2025-03-19 13:20:17,322 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:20:17,416 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:20:17,417 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-20-17_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:20:17,417 - INFO - Starting training pipeline
2025-03-19 13:20:17,417 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:20:17,953 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:20:17,953 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:20:18,051 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:20:18,051 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:20:18,526 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:20:18,672 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:20:20,985 - INFO - Moving model to cuda:0
2025-03-19 13:20:21,219 - INFO - Moving model to cuda:1
2025-03-19 13:20:21,244 - INFO - Loading datasets from 1 sources
2025-03-19 13:20:21,248 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:20:21,249 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:20:21,249 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:20:21,366 - INFO - Loading datasets from 1 sources
2025-03-19 13:20:21,370 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:20:21,371 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:20:21,371 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:20:22,249 - INFO - Created 32242 CPO samples
2025-03-19 13:20:22,379 - INFO - Created 32242 CPO samples
2025-03-19 13:20:22,387 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:20:22,520 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:21:05,528 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpzu4zx89w/test.c -o /tmp/tmpzu4zx89w/test.o
2025-03-19 13:21:05,551 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpzu4zx89w/test.o -laio -o /tmp/tmpzu4zx89w/a.out
2025-03-19 13:21:05,694 - INFO - Starting training
2025-03-19 13:21:48,847 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpuj496lpv/test.c -o /tmp/tmpuj496lpv/test.o
2025-03-19 13:21:48,870 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpuj496lpv/test.o -laio -o /tmp/tmpuj496lpv/a.out
2025-03-19 13:21:49,003 - INFO - Starting training
2025-03-19 13:29:47,042 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:29:47,043 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-29-46_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:29:47,044 - INFO - Starting training pipeline
2025-03-19 13:29:47,044 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:29:47,047 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:29:47,048 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-29-46_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:29:47,048 - INFO - Starting training pipeline
2025-03-19 13:29:47,048 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:29:47,661 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:29:47,661 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:29:47,959 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:29:47,959 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:29:48,223 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:29:48,548 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:29:50,668 - INFO - Moving model to cuda:0
2025-03-19 13:29:50,938 - INFO - Loading datasets from 1 sources
2025-03-19 13:29:50,942 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:29:50,944 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:29:50,944 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:29:50,999 - INFO - Moving model to cuda:0
2025-03-19 13:29:51,247 - INFO - Loading datasets from 1 sources
2025-03-19 13:29:51,251 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:29:51,253 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:29:51,253 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:29:51,939 - INFO - Created 32242 CPO samples
2025-03-19 13:29:52,077 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:29:52,260 - INFO - Created 32242 CPO samples
2025-03-19 13:29:52,399 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:30:35,494 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 452, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 404, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
2025-03-19 13:30:35,494 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 452, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 399, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
2025-03-19 13:33:20,180 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:33:20,181 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-33-20_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:33:20,181 - INFO - Starting training pipeline
2025-03-19 13:33:20,181 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:33:20,184 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:33:20,185 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-33-20_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:33:20,185 - INFO - Starting training pipeline
2025-03-19 13:33:20,186 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:33:20,816 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:33:20,816 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:33:20,821 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:33:20,821 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:33:21,388 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:33:21,391 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:33:23,843 - INFO - Moving model to cuda:0
2025-03-19 13:33:23,844 - INFO - Moving model to cuda:0
2025-03-19 13:33:24,182 - INFO - Loading datasets from 1 sources
2025-03-19 13:33:24,187 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:33:24,189 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:33:24,189 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:33:24,195 - INFO - Loading datasets from 1 sources
2025-03-19 13:33:24,201 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:33:24,203 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:33:24,203 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:33:25,203 - INFO - Created 32242 CPO samples
2025-03-19 13:33:25,228 - INFO - Created 32242 CPO samples
2025-03-19 13:33:25,302 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:33:25,371 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:34:07,825 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 452, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 404, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
2025-03-19 13:34:07,825 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 511, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 452, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 399, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
2025-03-19 13:37:13,574 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:37:13,575 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-37-13_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:37:13,575 - INFO - Starting training pipeline
2025-03-19 13:37:13,575 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:37:13,578 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:37:13,578 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-37-13_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:37:13,579 - INFO - Starting training pipeline
2025-03-19 13:37:13,579 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:37:14,187 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:37:14,187 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:37:14,187 - INFO - Distributed mode: Process 0 using device cuda:0
2025-03-19 13:37:14,277 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:37:14,277 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:37:14,277 - INFO - Distributed mode: Process 1 using device cuda:1
2025-03-19 13:37:14,778 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:37:14,867 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:37:17,317 - ERROR - Failed to load or move model: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-19 13:37:17,317 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 538, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 456, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 238, in initialize_model
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
           ^^^^^
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-19 13:37:17,505 - INFO - Model successfully moved to cuda:0
2025-03-19 13:37:17,505 - INFO - Loading datasets from 1 sources
2025-03-19 13:37:17,509 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:37:17,511 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:37:17,511 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:38:51,877 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:38:51,877 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-38-51_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:38:51,877 - INFO - Starting training pipeline
2025-03-19 13:38:51,877 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:38:51,913 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:38:51,914 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-38-51_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:38:51,914 - INFO - Starting training pipeline
2025-03-19 13:38:51,915 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:38:52,448 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:38:52,448 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:38:52,538 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:38:52,538 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:38:53,097 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:38:53,837 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:38:55,541 - INFO - Moving model to cuda:0
2025-03-19 13:38:55,808 - INFO - Loading datasets from 1 sources
2025-03-19 13:38:55,812 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:38:55,814 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:38:55,814 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:38:56,304 - INFO - Moving model to cuda:0
2025-03-19 13:38:56,543 - INFO - Loading datasets from 1 sources
2025-03-19 13:38:56,550 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:38:56,551 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:38:56,551 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:38:56,804 - INFO - Created 32242 CPO samples
2025-03-19 13:38:56,945 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:38:57,551 - INFO - Created 32242 CPO samples
2025-03-19 13:38:57,654 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:39:19,179 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:39:19,180 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-39-19_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:39:19,181 - INFO - Starting training pipeline
2025-03-19 13:39:19,181 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:39:19,182 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:39:19,183 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-39-19_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:39:19,184 - INFO - Starting training pipeline
2025-03-19 13:39:19,184 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:39:19,761 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:39:19,761 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:39:19,841 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:39:19,841 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:39:20,337 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:39:20,403 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:39:22,789 - INFO - Process 0 using device cuda:0
2025-03-19 13:39:22,846 - INFO - Process 1 using device cuda:1
2025-03-19 13:39:22,846 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 515, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 433, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 224, in initialize_model
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
           ^^^^^
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-19 13:39:23,055 - INFO - Loading datasets from 1 sources
2025-03-19 13:39:23,059 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:39:23,061 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:39:23,061 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:40:29,726 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:40:29,726 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-40-29_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:40:29,726 - INFO - Starting training pipeline
2025-03-19 13:40:29,726 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:40:29,819 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:40:29,820 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-40-29_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:40:29,821 - INFO - Starting training pipeline
2025-03-19 13:40:29,821 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:40:30,287 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:40:30,288 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:40:30,392 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:40:30,393 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:40:30,845 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:40:30,942 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:40:33,289 - INFO - Moving model to cuda:0
2025-03-19 13:40:33,391 - INFO - Moving model to cuda:1
2025-03-19 13:40:33,540 - INFO - Loading datasets from 1 sources
2025-03-19 13:40:33,544 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:40:33,545 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:40:33,545 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:40:33,555 - INFO - Loading datasets from 1 sources
2025-03-19 13:40:33,560 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:40:33,562 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:40:33,562 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:40:34,546 - INFO - Created 32242 CPO samples
2025-03-19 13:40:34,564 - INFO - Created 32242 CPO samples
2025-03-19 13:40:34,688 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:40:34,708 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:41:18,114 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpp8q88e3p/test.c -o /tmp/tmpp8q88e3p/test.o
2025-03-19 13:41:18,136 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpp8q88e3p/test.o -laio -o /tmp/tmpp8q88e3p/a.out
2025-03-19 13:41:18,282 - INFO - Starting training
2025-03-19 13:42:01,925 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpqipy357i/test.c -o /tmp/tmpqipy357i/test.o
2025-03-19 13:42:01,948 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpqipy357i/test.o -laio -o /tmp/tmpqipy357i/a.out
2025-03-19 13:42:02,091 - INFO - Starting training
2025-03-19 13:47:13,649 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:47:13,650 - INFO - Training Arguments: TrainingConfig(
_n_gpu=2,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-47-13_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:47:13,651 - INFO - Starting training pipeline
2025-03-19 13:47:13,651 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:47:14,738 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:47:14,739 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:47:15,530 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:47:17,987 - INFO - Moving model to cuda:0
2025-03-19 13:47:18,518 - INFO - Loading datasets from 1 sources
2025-03-19 13:47:18,531 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:47:18,540 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:47:18,540 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:47:25,390 - INFO - Created 32242 CPO samples
2025-03-19 13:47:25,546 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:51:31,716 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:51:31,717 - INFO - Training Arguments: TrainingConfig(
_n_gpu=2,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-51-31_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:51:31,718 - INFO - Starting training pipeline
2025-03-19 13:51:31,718 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:51:32,506 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:51:32,506 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:51:33,282 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:51:35,739 - INFO - Moving model to cuda:0
2025-03-19 13:51:36,262 - INFO - Loading datasets from 1 sources
2025-03-19 13:51:36,276 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:51:36,285 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:51:36,285 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:51:43,230 - INFO - Created 32242 CPO samples
2025-03-19 13:51:43,394 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:53:48,209 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:53:48,211 - INFO - Training Arguments: TrainingConfig(
_n_gpu=2,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-53-48_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:53:48,211 - INFO - Starting training pipeline
2025-03-19 13:53:48,211 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:53:49,035 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:53:49,035 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:53:49,914 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:53:52,325 - INFO - Moving model to cuda:0
2025-03-19 13:53:52,603 - INFO - Loading datasets from 1 sources
2025-03-19 13:53:52,616 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:53:52,625 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:53:52,625 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:53:59,408 - INFO - Created 32242 CPO samples
2025-03-19 13:53:59,568 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 13:59:28,753 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 13:59:28,755 - INFO - Training Arguments: TrainingConfig(
_n_gpu=2,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_13-59-28_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 13:59:28,755 - INFO - Starting training pipeline
2025-03-19 13:59:28,755 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:59:29,506 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 13:59:29,507 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 13:59:30,238 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 13:59:32,623 - INFO - Moving model to cuda:0
2025-03-19 13:59:32,901 - INFO - Loading datasets from 1 sources
2025-03-19 13:59:32,915 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 13:59:32,930 - INFO - Combined dataset contains 6373 examples
2025-03-19 13:59:32,930 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 13:59:39,635 - INFO - Created 32242 CPO samples
2025-03-19 13:59:39,793 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:04:35,605 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:04:35,606 - INFO - Training Arguments: TrainingConfig(
_n_gpu=2,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-04-35_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:04:35,606 - INFO - Starting training pipeline
2025-03-19 14:04:35,606 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:04:36,347 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:04:36,347 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:04:37,055 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:04:39,431 - INFO - Moving model to cuda:0
2025-03-19 14:04:39,710 - INFO - Loading datasets from 1 sources
2025-03-19 14:04:39,726 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:04:39,736 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:04:39,736 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:04:46,460 - INFO - Created 32242 CPO samples
2025-03-19 14:04:46,626 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:07:48,460 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:07:48,460 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-07-48_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:07:48,460 - INFO - Starting training pipeline
2025-03-19 14:07:48,460 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:07:48,466 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:07:48,467 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-07-48_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:07:48,468 - INFO - Starting training pipeline
2025-03-19 14:07:48,468 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:07:49,062 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:07:49,062 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:07:49,076 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:07:49,076 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:07:49,642 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:07:49,700 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:07:52,105 - INFO - Moving model to cuda:0
2025-03-19 14:07:52,149 - INFO - Moving model to cuda:0
2025-03-19 14:07:52,454 - INFO - Loading datasets from 1 sources
2025-03-19 14:07:52,458 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:07:52,459 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:07:52,459 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:07:52,461 - INFO - Loading datasets from 1 sources
2025-03-19 14:07:52,465 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:07:52,468 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:07:52,468 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:07:53,457 - INFO - Created 32242 CPO samples
2025-03-19 14:07:53,492 - INFO - Created 32242 CPO samples
2025-03-19 14:07:53,605 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:07:53,633 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:08:36,564 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 518, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 453, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 404, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
2025-03-19 14:08:36,565 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 518, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 453, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 399, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
2025-03-19 14:13:22,470 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:13:22,471 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-13-22_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:13:22,471 - INFO - Starting training pipeline
2025-03-19 14:13:22,471 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:13:22,476 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:13:22,477 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-13-22_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:13:22,478 - INFO - Starting training pipeline
2025-03-19 14:13:22,478 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:13:23,172 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:13:23,172 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:13:23,185 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:13:23,185 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:13:23,913 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:13:24,217 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:13:26,352 - INFO - Using single GPU mode with device cuda:0
2025-03-19 14:13:26,605 - INFO - Loading datasets from 1 sources
2025-03-19 14:13:26,609 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:13:26,611 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:13:26,611 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:13:26,666 - INFO - Using single GPU mode with device cuda:0
2025-03-19 14:13:26,912 - INFO - Loading datasets from 1 sources
2025-03-19 14:13:26,917 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:13:26,919 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:13:26,919 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:13:27,626 - INFO - Created 32242 CPO samples
2025-03-19 14:13:27,763 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:13:27,938 - INFO - Created 32242 CPO samples
2025-03-19 14:13:28,077 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:14:10,865 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 454, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 399, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
2025-03-19 14:14:10,865 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 454, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 404, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
2025-03-19 14:24:26,580 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:24:26,581 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-24-26_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:24:26,581 - INFO - Starting training pipeline
2025-03-19 14:24:26,581 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:24:27,231 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:24:27,231 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:24:27,818 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:24:30,220 - INFO - Using single GPU mode with device cuda:0
2025-03-19 14:24:30,492 - INFO - Loading datasets from 1 sources
2025-03-19 14:24:30,496 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:24:30,497 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:24:30,497 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:24:31,478 - INFO - Created 32242 CPO samples
2025-03-19 14:24:31,617 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:25:12,847 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmphycq7s0h/test.c -o /tmp/tmphycq7s0h/test.o
2025-03-19 14:25:12,871 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmphycq7s0h/test.o -laio -o /tmp/tmphycq7s0h/a.out
2025-03-19 14:25:13,008 - INFO - Starting training
2025-03-19 14:25:53,362 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-19 14:25:53,363 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar19_14-25-53_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-19 14:25:53,364 - INFO - Starting training pipeline
2025-03-19 14:25:53,364 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:25:53,993 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-19 14:25:53,993 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-19 14:25:54,573 - INFO - Resizing token embeddings from 151936 to 151648
2025-03-19 14:25:56,942 - INFO - Using single GPU mode with device cuda:0
2025-03-19 14:25:57,215 - INFO - Loading datasets from 1 sources
2025-03-19 14:25:57,219 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-19 14:25:57,221 - INFO - Combined dataset contains 6373 examples
2025-03-19 14:25:57,221 - INFO - Preparing CPO dataset from 6373 examples
2025-03-19 14:25:58,189 - INFO - Created 32242 CPO samples
2025-03-19 14:25:58,350 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-19 14:26:40,250 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp_2oyqmua/test.c -o /tmp/tmp_2oyqmua/test.o
2025-03-19 14:26:40,275 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp_2oyqmua/test.o -laio -o /tmp/tmp_2oyqmua/a.out
2025-03-19 14:26:40,408 - INFO - Starting training
2025-03-19 15:27:12,260 - INFO - Training completed successfully
2025-03-19 15:27:13,331 - INFO - Total training time: 60.55 minutes
2025-03-21 13:21:25,877 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpbhznydly/test.c -o /tmp/tmpbhznydly/test.o
2025-03-21 13:21:25,885 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmph_j6569i/test.c -o /tmp/tmph_j6569i/test.o
2025-03-21 13:21:25,898 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpbhznydly/test.o -laio -o /tmp/tmpbhznydly/a.out
2025-03-21 13:21:25,900 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmph_j6569i/test.o -laio -o /tmp/tmph_j6569i/a.out
2025-03-21 13:21:26,223 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:21:26,223 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct/runs/Mar21_13-21-25_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Meta-Llama-3.1-8B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:21:26,223 - INFO - Starting training pipeline
2025-03-21 13:21:26,223 - INFO - Initializing tokenizer from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:21:26,223 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:21:26,224 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct/runs/Mar21_13-21-25_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Meta-Llama-3.1-8B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:21:26,224 - INFO - Starting training pipeline
2025-03-21 13:21:26,224 - INFO - Initializing tokenizer from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:21:27,046 - ERROR - Training pipeline failed: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.
403 Client Error. (Request ID: Root=1-67dcf756-6d01fa612c3dbcca0818a78e;d8fae624-d6fe-49ae-8ecc-23ac6b8822db)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.
Traceback (most recent call last):
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-67dcf756-6d01fa612c3dbcca0818a78e;d8fae624-d6fe-49ae-8ecc-23ac6b8822db)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 430, in train
    tokenizer = initialize_tokenizer(args.model_name_or_path, special_tokens)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 170, in initialize_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 901, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1075, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/configuration_utils.py", line 594, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/configuration_utils.py", line 653, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/hub.py", line 360, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.
403 Client Error. (Request ID: Root=1-67dcf756-6d01fa612c3dbcca0818a78e;d8fae624-d6fe-49ae-8ecc-23ac6b8822db)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.
2025-03-21 13:21:27,079 - ERROR - Training pipeline failed: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.
403 Client Error. (Request ID: Root=1-67dcf756-33162db40995ec0716367434;11510665-318f-496b-a21a-6c242a693b41)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.
Traceback (most recent call last):
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-67dcf756-33162db40995ec0716367434;11510665-318f-496b-a21a-6c242a693b41)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 430, in train
    tokenizer = initialize_tokenizer(args.model_name_or_path, special_tokens)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 170, in initialize_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 901, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1075, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/configuration_utils.py", line 594, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/configuration_utils.py", line 653, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/hub.py", line 360, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.
403 Client Error. (Request ID: Root=1-67dcf756-33162db40995ec0716367434;11510665-318f-496b-a21a-6c242a693b41)

Cannot access gated repo for url https://hf-mirror.com/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.
Your request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo's authors.
2025-03-21 13:22:29,370 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp35w7fuby/test.c -o /tmp/tmp35w7fuby/test.o
2025-03-21 13:22:29,373 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmppn4l4rnz/test.c -o /tmp/tmppn4l4rnz/test.o
2025-03-21 13:22:29,391 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp35w7fuby/test.o -laio -o /tmp/tmp35w7fuby/a.out
2025-03-21 13:22:29,391 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmppn4l4rnz/test.o -laio -o /tmp/tmppn4l4rnz/a.out
2025-03-21 13:22:29,718 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:22:29,718 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct/runs/Mar21_13-22-29_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Meta-Llama-3.1-8B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:22:29,718 - INFO - Starting training pipeline
2025-03-21 13:22:29,718 - INFO - Initializing tokenizer from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:22:29,720 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='meta-llama/Meta-Llama-3.1-8B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:22:29,720 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct/runs/Mar21_13-22-29_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Meta-Llama-3.1-8B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Meta-Llama-3.1-8B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:22:29,720 - INFO - Starting training pipeline
2025-03-21 13:22:29,720 - INFO - Initializing tokenizer from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:22:36,566 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:22:36,567 - INFO - Initializing model from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:22:36,628 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:22:36,628 - INFO - Initializing model from meta-llama/Meta-Llama-3.1-8B-Instruct
2025-03-21 13:23:33,912 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpunszrtsz/test.c -o /tmp/tmpunszrtsz/test.o
2025-03-21 13:23:33,932 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpo1be6n_q/test.c -o /tmp/tmpo1be6n_q/test.o
2025-03-21 13:23:33,935 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpunszrtsz/test.o -laio -o /tmp/tmpunszrtsz/a.out
2025-03-21 13:23:33,949 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpo1be6n_q/test.o -laio -o /tmp/tmpo1be6n_q/a.out
2025-03-21 13:23:34,268 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:23:34,268 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-23-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:23:34,268 - INFO - Starting training pipeline
2025-03-21 13:23:34,268 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:23:34,272 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:23:34,272 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-23-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:23:34,272 - INFO - Starting training pipeline
2025-03-21 13:23:34,272 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:23:34,614 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:23:34,614 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:23:34,614 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:23:34,615 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:23:34,660 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 431, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:26:41,970 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp6l2hn6eg/test.c -o /tmp/tmp6l2hn6eg/test.o
2025-03-21 13:26:41,977 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpd7_etxsq/test.c -o /tmp/tmpd7_etxsq/test.o
2025-03-21 13:26:41,989 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp6l2hn6eg/test.o -laio -o /tmp/tmp6l2hn6eg/a.out
2025-03-21 13:26:41,992 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpd7_etxsq/test.o -laio -o /tmp/tmpd7_etxsq/a.out
2025-03-21 13:26:42,317 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:26:42,317 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-26-41_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:26:42,317 - INFO - Starting training pipeline
2025-03-21 13:26:42,317 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:26:42,318 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:26:42,318 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-26-41_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:26:42,318 - INFO - Starting training pipeline
2025-03-21 13:26:42,318 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:26:42,590 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:26:42,590 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:26:42,594 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:26:42,594 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:26:43,603 - INFO - Resizing token embeddings from 128256 to 128259
2025-03-21 13:26:43,613 - INFO - Resizing token embeddings from 128256 to 128259
2025-03-21 13:27:48,545 - INFO - Using single GPU mode with device cuda:0
2025-03-21 13:27:48,545 - INFO - Using single GPU mode with device cuda:0
2025-03-21 13:27:56,790 - INFO - Loading datasets from 1 sources
2025-03-21 13:27:56,794 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:27:56,797 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:27:56,797 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:27:56,801 - INFO - Loading datasets from 1 sources
2025-03-21 13:27:56,805 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:27:56,808 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:27:56,808 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:27:58,258 - INFO - Created 32242 CPO samples
2025-03-21 13:27:58,430 - INFO - Created 32242 CPO samples
2025-03-21 13:27:58,448 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:27:58,564 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:28:21,010 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 454, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 399, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 55000
2025-03-21 13:28:21,010 - ERROR - Training pipeline failed: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 454, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 328, in __init__
    with PartialState().local_main_process_first():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 535, in local_main_process_first
    yield from self._goes_first(self.is_local_main_process)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 404, in _goes_first
    self.wait_for_everyone()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/state.py", line 393, in wait_for_everyone
    torch.distributed.barrier()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py", line 4551, in barrier
    work = group.barrier(opts=opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 55000
2025-03-21 13:31:47,597 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpkpydypem/test.c -o /tmp/tmpkpydypem/test.o
2025-03-21 13:31:47,597 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpgvdnztxv/test.c -o /tmp/tmpgvdnztxv/test.o
2025-03-21 13:31:47,616 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpgvdnztxv/test.o -laio -o /tmp/tmpgvdnztxv/a.out
2025-03-21 13:31:47,616 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpkpydypem/test.o -laio -o /tmp/tmpkpydypem/a.out
2025-03-21 13:31:47,944 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:31:47,944 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-31-47_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:31:47,944 - INFO - Starting training pipeline
2025-03-21 13:31:47,944 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:31:47,947 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:31:47,947 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-31-47_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:31:47,947 - INFO - Starting training pipeline
2025-03-21 13:31:47,947 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:31:48,225 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:31:48,225 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:31:48,227 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:31:48,227 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:31:48,255 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 431, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:32:26,959 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprj7pxhst/test.c -o /tmp/tmprj7pxhst/test.o
2025-03-21 13:32:26,964 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpffplsvzb/test.c -o /tmp/tmpffplsvzb/test.o
2025-03-21 13:32:26,977 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprj7pxhst/test.o -laio -o /tmp/tmprj7pxhst/a.out
2025-03-21 13:32:26,980 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpffplsvzb/test.o -laio -o /tmp/tmpffplsvzb/a.out
2025-03-21 13:32:27,314 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:32:27,315 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-32-26_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:32:27,315 - INFO - Starting training pipeline
2025-03-21 13:32:27,315 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:32:27,315 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:32:27,316 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-32-26_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:32:27,316 - INFO - Starting training pipeline
2025-03-21 13:32:27,316 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:32:27,610 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:32:27,610 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:32:27,616 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:32:27,616 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:32:27,634 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 519, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 431, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:36:37,393 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp2p24skpb/test.c -o /tmp/tmp2p24skpb/test.o
2025-03-21 13:36:37,396 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprv33munz/test.c -o /tmp/tmprv33munz/test.o
2025-03-21 13:36:37,412 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprv33munz/test.o -laio -o /tmp/tmprv33munz/a.out
2025-03-21 13:36:37,412 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp2p24skpb/test.o -laio -o /tmp/tmp2p24skpb/a.out
2025-03-21 13:36:37,739 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:36:37,739 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-36-37_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:36:37,739 - INFO - Starting training pipeline
2025-03-21 13:36:37,739 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:36:37,744 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:36:37,744 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-36-37_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:36:37,745 - INFO - Starting training pipeline
2025-03-21 13:36:37,745 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:36:38,016 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:36:38,016 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:36:38,026 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:36:38,026 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:36:38,044 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 520, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 432, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:37:06,935 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpom_pog1a/test.c -o /tmp/tmpom_pog1a/test.o
2025-03-21 13:37:06,935 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpev22ucvr/test.c -o /tmp/tmpev22ucvr/test.o
2025-03-21 13:37:06,956 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpev22ucvr/test.o -laio -o /tmp/tmpev22ucvr/a.out
2025-03-21 13:37:06,956 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpom_pog1a/test.o -laio -o /tmp/tmpom_pog1a/a.out
2025-03-21 13:37:07,289 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:37:07,289 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:37:07,289 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-37-06_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:37:07,289 - INFO - Starting training pipeline
2025-03-21 13:37:07,289 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:37:07,289 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-37-06_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:37:07,289 - INFO - Starting training pipeline
2025-03-21 13:37:07,289 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:37:07,575 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:37:07,575 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:37:07,577 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:37:07,577 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:37:07,594 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 425, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:39:03,634 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprvce55g9/test.c -o /tmp/tmprvce55g9/test.o
2025-03-21 13:39:03,635 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp0tl_4m8l/test.c -o /tmp/tmp0tl_4m8l/test.o
2025-03-21 13:39:03,654 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprvce55g9/test.o -laio -o /tmp/tmprvce55g9/a.out
2025-03-21 13:39:03,654 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp0tl_4m8l/test.o -laio -o /tmp/tmp0tl_4m8l/a.out
2025-03-21 13:39:03,980 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:39:03,980 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-39-03_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:39:03,980 - INFO - Starting training pipeline
2025-03-21 13:39:03,980 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:39:03,981 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:39:03,981 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-39-03_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:39:03,981 - INFO - Starting training pipeline
2025-03-21 13:39:03,982 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:39:04,257 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:39:04,257 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:39:04,260 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:39:04,261 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:39:04,275 - ERROR - Training pipeline failed: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 512, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 424, in train
    model, peft_config = initialize_model(args, tokenizer)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 209, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4164, in from_pretrained
    deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 979, in __init__
    get_accelerator().set_device(self.local_device)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 67, in set_device
    torch.cuda.set_device(device_index)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/cuda/__init__.py", line 476, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-03-21 13:40:49,529 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmphnyxzooz/test.c -o /tmp/tmphnyxzooz/test.o
2025-03-21 13:40:49,530 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprnvgn1l4/test.c -o /tmp/tmprnvgn1l4/test.o
2025-03-21 13:40:49,548 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmphnyxzooz/test.o -laio -o /tmp/tmphnyxzooz/a.out
2025-03-21 13:40:49,549 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprnvgn1l4/test.o -laio -o /tmp/tmprnvgn1l4/a.out
2025-03-21 13:40:49,904 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:40:49,905 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-40-49_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:40:49,905 - INFO - Starting training pipeline
2025-03-21 13:40:49,905 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:40:50,002 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:40:50,003 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-40-49_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:40:50,003 - INFO - Starting training pipeline
2025-03-21 13:40:50,003 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:40:50,187 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:40:50,188 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:40:50,273 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:40:50,273 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:40:52,742 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:40:55,948 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:40:56,638 - INFO - Loading datasets from 1 sources
2025-03-21 13:40:56,640 - INFO - Loading datasets from 1 sources
2025-03-21 13:40:56,641 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:40:56,643 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:40:56,643 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:40:56,645 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:40:56,647 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:40:56,647 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:40:57,736 - INFO - Created 32242 CPO samples
2025-03-21 13:40:57,761 - INFO - Created 32242 CPO samples
2025-03-21 13:40:57,920 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:40:57,958 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:41:35,233 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpy_hvyngd/test.c -o /tmp/tmpy_hvyngd/test.o
2025-03-21 13:41:35,253 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpy_hvyngd/test.o -laio -o /tmp/tmpy_hvyngd/a.out
2025-03-21 13:41:35,258 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpsnxb92_v/test.c -o /tmp/tmpsnxb92_v/test.o
2025-03-21 13:41:35,274 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpsnxb92_v/test.o -laio -o /tmp/tmpsnxb92_v/a.out
2025-03-21 13:41:35,616 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:41:35,616 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-41-35_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:41:35,616 - INFO - Starting training pipeline
2025-03-21 13:41:35,616 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:41:35,730 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:41:35,730 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-41-35_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:41:35,730 - INFO - Starting training pipeline
2025-03-21 13:41:35,730 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:41:35,896 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:41:35,896 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:41:35,996 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:41:35,996 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:41:37,855 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:41:40,989 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:41:41,679 - INFO - Loading datasets from 1 sources
2025-03-21 13:41:41,683 - INFO - Loading datasets from 1 sources
2025-03-21 13:41:41,684 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:41:41,686 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:41:41,686 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:41:41,687 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:41:41,688 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:41:41,688 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:41:42,789 - INFO - Created 32242 CPO samples
2025-03-21 13:41:42,791 - INFO - Created 32242 CPO samples
2025-03-21 13:41:42,979 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:41:42,987 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:42:04,747 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 512, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 447, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:43:56,647 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp6mt6ox31/test.c -o /tmp/tmp6mt6ox31/test.o
2025-03-21 13:43:56,648 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpk7vuf479/test.c -o /tmp/tmpk7vuf479/test.o
2025-03-21 13:43:56,666 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp6mt6ox31/test.o -laio -o /tmp/tmp6mt6ox31/a.out
2025-03-21 13:43:56,666 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpk7vuf479/test.o -laio -o /tmp/tmpk7vuf479/a.out
2025-03-21 13:43:57,058 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:43:57,059 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-43-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:43:57,059 - INFO - Starting training pipeline
2025-03-21 13:43:57,059 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:43:57,152 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:43:57,152 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-43-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:43:57,152 - INFO - Starting training pipeline
2025-03-21 13:43:57,152 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:43:57,363 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:43:57,363 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:43:57,449 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:43:57,449 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:43:59,321 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:44:02,516 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:44:03,208 - INFO - Loading datasets from 1 sources
2025-03-21 13:44:03,213 - INFO - Loading datasets from 1 sources
2025-03-21 13:44:03,215 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:44:03,216 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:44:03,216 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:44:03,217 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:44:03,218 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:44:03,218 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:44:04,329 - INFO - Created 32242 CPO samples
2025-03-21 13:44:04,333 - INFO - Created 32242 CPO samples
2025-03-21 13:44:04,527 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:44:04,528 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:44:26,662 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 512, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 447, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:45:30,222 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmppt3i4vhv/test.c -o /tmp/tmppt3i4vhv/test.o
2025-03-21 13:45:30,227 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp314nravm/test.c -o /tmp/tmp314nravm/test.o
2025-03-21 13:45:30,244 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmppt3i4vhv/test.o -laio -o /tmp/tmppt3i4vhv/a.out
2025-03-21 13:45:30,247 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp314nravm/test.o -laio -o /tmp/tmp314nravm/a.out
2025-03-21 13:45:30,605 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:45:30,605 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-45-30_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:45:30,605 - INFO - Starting training pipeline
2025-03-21 13:45:30,605 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:45:30,699 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:45:30,699 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-45-30_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:45:30,699 - INFO - Starting training pipeline
2025-03-21 13:45:30,699 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:45:30,889 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:45:30,889 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:45:30,975 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:45:30,975 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:45:32,844 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:45:35,969 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:45:36,660 - INFO - Loading datasets from 1 sources
2025-03-21 13:45:36,662 - INFO - Loading datasets from 1 sources
2025-03-21 13:45:36,664 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:45:36,666 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:45:36,666 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:45:36,667 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:45:36,670 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:45:36,670 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:45:37,768 - INFO - Created 32242 CPO samples
2025-03-21 13:45:37,774 - INFO - Created 32242 CPO samples
2025-03-21 13:45:37,964 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:45:37,979 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:46:00,438 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 512, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 447, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:47:36,839 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmppov8rq5f/test.c -o /tmp/tmppov8rq5f/test.o
2025-03-21 13:47:36,841 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpmcz13enk/test.c -o /tmp/tmpmcz13enk/test.o
2025-03-21 13:47:36,859 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmppov8rq5f/test.o -laio -o /tmp/tmppov8rq5f/a.out
2025-03-21 13:47:36,859 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpmcz13enk/test.o -laio -o /tmp/tmpmcz13enk/a.out
2025-03-21 13:47:37,228 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:47:37,228 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-47-36_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:47:37,229 - INFO - Starting training pipeline
2025-03-21 13:47:37,229 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:47:37,310 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:47:37,310 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-47-36_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:47:37,310 - INFO - Starting training pipeline
2025-03-21 13:47:37,310 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:47:37,512 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:47:37,512 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:47:37,582 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:47:37,582 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:47:39,477 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:47:42,648 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:47:43,345 - INFO - Loading datasets from 1 sources
2025-03-21 13:47:43,346 - INFO - Loading datasets from 1 sources
2025-03-21 13:47:43,349 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:47:43,350 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:47:43,351 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:47:43,351 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:47:43,352 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:47:43,352 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:47:44,448 - INFO - Created 32242 CPO samples
2025-03-21 13:47:44,469 - INFO - Created 32242 CPO samples
2025-03-21 13:47:44,632 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:47:44,675 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:48:06,701 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 512, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 447, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:48:44,905 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpeeityxms/test.c -o /tmp/tmpeeityxms/test.o
2025-03-21 13:48:44,906 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp4nfl2ea8/test.c -o /tmp/tmp4nfl2ea8/test.o
2025-03-21 13:48:44,927 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp4nfl2ea8/test.o -laio -o /tmp/tmp4nfl2ea8/a.out
2025-03-21 13:48:44,927 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpeeityxms/test.o -laio -o /tmp/tmpeeityxms/a.out
2025-03-21 13:48:45,268 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:48:45,268 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-48-44_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:48:45,268 - INFO - Starting training pipeline
2025-03-21 13:48:45,268 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:48:45,367 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:48:45,368 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-48-44_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:48:45,368 - INFO - Starting training pipeline
2025-03-21 13:48:45,368 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:48:45,547 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:48:45,547 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:48:45,637 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:48:45,637 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:48:47,505 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:48:50,641 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:48:51,334 - INFO - Loading datasets from 1 sources
2025-03-21 13:48:51,336 - INFO - Loading datasets from 1 sources
2025-03-21 13:48:51,338 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:48:51,339 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:48:51,339 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:48:51,340 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:48:51,341 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:48:51,341 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:48:52,456 - INFO - Created 32242 CPO samples
2025-03-21 13:48:52,466 - INFO - Created 32242 CPO samples
2025-03-21 13:48:52,653 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:48:52,665 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:49:14,356 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:51:23,048 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp1ucfhp5w/test.c -o /tmp/tmp1ucfhp5w/test.o
2025-03-21 13:51:23,071 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp1ucfhp5w/test.o -laio -o /tmp/tmp1ucfhp5w/a.out
2025-03-21 13:51:23,081 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmph3pyz_m9/test.c -o /tmp/tmph3pyz_m9/test.o
2025-03-21 13:51:23,097 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmph3pyz_m9/test.o -laio -o /tmp/tmph3pyz_m9/a.out
2025-03-21 13:51:23,442 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:51:23,442 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-51-22_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:51:23,442 - INFO - Starting training pipeline
2025-03-21 13:51:23,442 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:51:23,522 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:51:23,522 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-51-22_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:51:23,523 - INFO - Starting training pipeline
2025-03-21 13:51:23,523 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:51:23,725 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:51:23,725 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:51:23,791 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:51:23,791 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:51:25,662 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:51:28,812 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:51:29,499 - INFO - Loading datasets from 1 sources
2025-03-21 13:51:29,502 - INFO - Loading datasets from 1 sources
2025-03-21 13:51:29,502 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:51:29,504 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:51:29,504 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:51:29,505 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:51:29,507 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:51:29,507 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:51:30,575 - INFO - Created 32242 CPO samples
2025-03-21 13:51:30,585 - INFO - Created 32242 CPO samples
2025-03-21 13:51:30,776 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:51:30,778 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:51:30,811 - ERROR - Training pipeline failed: CPOTrainer.__init__() got an unexpected keyword argument 'gradient_accumulation_steps'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 514, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
TypeError: CPOTrainer.__init__() got an unexpected keyword argument 'gradient_accumulation_steps'
2025-03-21 13:51:30,814 - ERROR - Training pipeline failed: CPOTrainer.__init__() got an unexpected keyword argument 'gradient_accumulation_steps'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 514, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
TypeError: CPOTrainer.__init__() got an unexpected keyword argument 'gradient_accumulation_steps'
2025-03-21 13:52:56,903 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpdbmv_uu9/test.c -o /tmp/tmpdbmv_uu9/test.o
2025-03-21 13:52:56,928 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpdbmv_uu9/test.o -laio -o /tmp/tmpdbmv_uu9/a.out
2025-03-21 13:52:56,932 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp39n7q1fp/test.c -o /tmp/tmp39n7q1fp/test.o
2025-03-21 13:52:56,947 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp39n7q1fp/test.o -laio -o /tmp/tmp39n7q1fp/a.out
2025-03-21 13:52:57,298 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:52:57,299 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-52-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:52:57,299 - INFO - Starting training pipeline
2025-03-21 13:52:57,299 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:52:57,395 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:52:57,395 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-52-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:52:57,395 - INFO - Starting training pipeline
2025-03-21 13:52:57,395 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:52:57,580 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:52:57,580 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:52:57,665 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:52:57,665 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:52:59,519 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:53:02,672 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:53:03,359 - INFO - Loading datasets from 1 sources
2025-03-21 13:53:03,360 - INFO - Loading datasets from 1 sources
2025-03-21 13:53:03,363 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:53:03,364 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:53:03,365 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:53:03,365 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:53:03,366 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:53:03,366 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:53:04,467 - INFO - Created 32242 CPO samples
2025-03-21 13:53:04,476 - INFO - Created 32242 CPO samples
2025-03-21 13:53:04,669 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:53:04,670 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:53:26,094 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 514, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:53:56,836 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpuvdo8j3o/test.c -o /tmp/tmpuvdo8j3o/test.o
2025-03-21 13:53:56,842 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpxd2b1bj8/test.c -o /tmp/tmpxd2b1bj8/test.o
2025-03-21 13:53:56,855 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpuvdo8j3o/test.o -laio -o /tmp/tmpuvdo8j3o/a.out
2025-03-21 13:53:56,859 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpxd2b1bj8/test.o -laio -o /tmp/tmpxd2b1bj8/a.out
2025-03-21 13:53:57,216 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:53:57,216 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-53-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:53:57,216 - INFO - Starting training pipeline
2025-03-21 13:53:57,216 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:53:57,317 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:53:57,317 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-53-56_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:53:57,318 - INFO - Starting training pipeline
2025-03-21 13:53:57,318 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:53:57,511 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:53:57,511 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:53:57,606 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:53:57,606 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:53:59,521 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:54:02,683 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:54:03,372 - INFO - Loading datasets from 1 sources
2025-03-21 13:54:03,376 - INFO - Loading datasets from 1 sources
2025-03-21 13:54:03,379 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:54:03,379 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:54:03,381 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:54:03,381 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:54:03,381 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:54:03,381 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:54:04,498 - INFO - Created 32242 CPO samples
2025-03-21 13:54:04,505 - INFO - Created 32242 CPO samples
2025-03-21 13:54:04,689 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:54:04,702 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:54:26,640 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 515, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 13:55:19,867 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpwridraua/test.c -o /tmp/tmpwridraua/test.o
2025-03-21 13:55:19,868 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp0a73futt/test.c -o /tmp/tmp0a73futt/test.o
2025-03-21 13:55:19,880 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp0a73futt/test.o -laio -o /tmp/tmp0a73futt/a.out
2025-03-21 13:55:19,880 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpwridraua/test.o -laio -o /tmp/tmpwridraua/a.out
2025-03-21 13:55:20,244 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:55:20,244 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-55-19_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:55:20,244 - INFO - Starting training pipeline
2025-03-21 13:55:20,244 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:55:20,339 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:55:20,339 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-55-19_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:55:20,339 - INFO - Starting training pipeline
2025-03-21 13:55:20,339 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:55:20,542 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:55:20,542 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:55:20,628 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:55:20,628 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:55:22,527 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:55:25,766 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:55:26,460 - INFO - Loading datasets from 1 sources
2025-03-21 13:55:26,463 - INFO - Loading datasets from 1 sources
2025-03-21 13:55:26,463 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:55:26,465 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:55:26,465 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:55:26,466 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:55:26,467 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:55:26,467 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:55:27,569 - INFO - Created 32242 CPO samples
2025-03-21 13:55:27,577 - INFO - Created 32242 CPO samples
2025-03-21 13:55:27,749 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:55:27,770 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:55:49,815 - ERROR - Training pipeline failed: name 'parse_choice_from_env_choice_from_env' is not defined
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 515, in __init__
    print(parse_choice_from_env("ACCELERATE_GRADIENT_ACCUMULATION_STEPS"))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'parse_choice_from_env_choice_from_env' is not defined
2025-03-21 13:56:05,373 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpv1r0hxzf/test.c -o /tmp/tmpv1r0hxzf/test.o
2025-03-21 13:56:05,374 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpa_w6_w96/test.c -o /tmp/tmpa_w6_w96/test.o
2025-03-21 13:56:05,394 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpv1r0hxzf/test.o -laio -o /tmp/tmpv1r0hxzf/a.out
2025-03-21 13:56:05,394 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpa_w6_w96/test.o -laio -o /tmp/tmpa_w6_w96/a.out
2025-03-21 13:56:05,750 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:56:05,750 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-56-05_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:56:05,750 - INFO - Starting training pipeline
2025-03-21 13:56:05,750 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:56:05,841 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 13:56:05,841 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_13-56-05_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 13:56:05,841 - INFO - Starting training pipeline
2025-03-21 13:56:05,841 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:56:06,027 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:56:06,027 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:56:06,111 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 13:56:06,111 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 13:56:07,972 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:56:11,136 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 13:56:11,828 - INFO - Loading datasets from 1 sources
2025-03-21 13:56:11,832 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:56:11,832 - INFO - Loading datasets from 1 sources
2025-03-21 13:56:11,833 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:56:11,833 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:56:11,835 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 13:56:11,836 - INFO - Combined dataset contains 6373 examples
2025-03-21 13:56:11,836 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 13:56:12,928 - INFO - Created 32242 CPO samples
2025-03-21 13:56:12,942 - INFO - Created 32242 CPO samples
2025-03-21 13:56:13,121 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:56:13,126 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 13:56:35,115 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 516, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 14:00:03,793 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp17dw75gb/test.c -o /tmp/tmp17dw75gb/test.o
2025-03-21 14:00:03,814 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp17dw75gb/test.o -laio -o /tmp/tmp17dw75gb/a.out
2025-03-21 14:00:03,824 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp3c73m_gi/test.c -o /tmp/tmp3c73m_gi/test.o
2025-03-21 14:00:03,841 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp3c73m_gi/test.o -laio -o /tmp/tmp3c73m_gi/a.out
2025-03-21 14:00:04,163 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:00:04,163 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-00-03_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:00:04,163 - INFO - Starting training pipeline
2025-03-21 14:00:04,163 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:00:04,290 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:00:04,290 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-00-03_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:00:04,290 - INFO - Starting training pipeline
2025-03-21 14:00:04,290 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:00:04,438 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:00:04,438 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:00:04,557 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:00:04,557 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:00:06,386 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:00:09,519 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:00:10,207 - INFO - Loading datasets from 1 sources
2025-03-21 14:00:10,212 - INFO - Loading datasets from 1 sources
2025-03-21 14:00:10,213 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:00:10,215 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:00:10,215 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:00:10,215 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:00:10,216 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:00:10,217 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:00:11,315 - INFO - Created 32242 CPO samples
2025-03-21 14:00:11,322 - INFO - Created 32242 CPO samples
2025-03-21 14:00:11,510 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:00:11,515 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:00:33,425 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 516, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 14:01:11,217 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpk1gqhuhg/test.c -o /tmp/tmpk1gqhuhg/test.o
2025-03-21 14:01:11,217 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp3ymo_mve/test.c -o /tmp/tmp3ymo_mve/test.o
2025-03-21 14:01:11,238 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp3ymo_mve/test.o -laio -o /tmp/tmp3ymo_mve/a.out
2025-03-21 14:01:11,238 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpk1gqhuhg/test.o -laio -o /tmp/tmpk1gqhuhg/a.out
2025-03-21 14:01:11,582 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:01:11,582 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-01-11_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:01:11,582 - INFO - Starting training pipeline
2025-03-21 14:01:11,582 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:01:11,677 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:01:11,678 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-01-11_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:01:11,678 - INFO - Starting training pipeline
2025-03-21 14:01:11,678 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:01:11,863 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:01:11,864 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:01:11,943 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:01:11,943 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:01:13,818 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:01:16,931 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:01:17,622 - INFO - Loading datasets from 1 sources
2025-03-21 14:01:17,625 - INFO - Loading datasets from 1 sources
2025-03-21 14:01:17,626 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:01:17,627 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:01:17,627 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:01:17,628 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:01:17,630 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:01:17,630 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:01:18,733 - INFO - Created 32242 CPO samples
2025-03-21 14:01:18,733 - INFO - Created 32242 CPO samples
2025-03-21 14:01:18,928 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:01:18,931 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:01:40,906 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 516, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 14:02:09,378 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpkk8c3gbb/test.c -o /tmp/tmpkk8c3gbb/test.o
2025-03-21 14:02:09,400 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpkk8c3gbb/test.o -laio -o /tmp/tmpkk8c3gbb/a.out
2025-03-21 14:02:09,728 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:02:09,728 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-02-09_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:02:09,728 - INFO - Starting training pipeline
2025-03-21 14:02:09,728 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:02:09,986 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:02:09,986 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:02:13,216 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:02:13,582 - INFO - Loading datasets from 1 sources
2025-03-21 14:02:13,585 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:02:13,587 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:02:13,587 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:02:14,671 - INFO - Created 32242 CPO samples
2025-03-21 14:02:14,857 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:02:36,867 - ERROR - Training pipeline failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 448, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 516, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-21 14:02:58,539 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp7f6zc2al/test.c -o /tmp/tmp7f6zc2al/test.o
2025-03-21 14:02:58,539 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpswsk3wrg/test.c -o /tmp/tmpswsk3wrg/test.o
2025-03-21 14:02:58,560 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpswsk3wrg/test.o -laio -o /tmp/tmpswsk3wrg/a.out
2025-03-21 14:02:58,560 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp7f6zc2al/test.o -laio -o /tmp/tmp7f6zc2al/a.out
2025-03-21 14:02:58,904 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:02:58,904 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-02-58_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:02:58,904 - INFO - Starting training pipeline
2025-03-21 14:02:58,904 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:02:59,005 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:02:59,005 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-02-58_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:02:59,005 - INFO - Starting training pipeline
2025-03-21 14:02:59,005 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:02:59,181 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:02:59,181 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:02:59,274 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:02:59,274 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:03:01,133 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:03:04,249 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:03:04,939 - INFO - Loading datasets from 1 sources
2025-03-21 14:03:04,943 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:03:04,943 - INFO - Loading datasets from 1 sources
2025-03-21 14:03:04,944 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:03:04,945 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:03:04,946 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:03:04,948 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:03:04,948 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:03:06,034 - INFO - Created 32242 CPO samples
2025-03-21 14:03:06,053 - INFO - Created 32242 CPO samples
2025-03-21 14:03:06,221 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:03:06,256 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:03:27,891 - INFO - Starting training
2025-03-21 14:03:41,539 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmps08cgrjr/test.c -o /tmp/tmps08cgrjr/test.o
2025-03-21 14:03:41,540 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpw2ufy9ss/test.c -o /tmp/tmpw2ufy9ss/test.o
2025-03-21 14:03:41,557 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmps08cgrjr/test.o -laio -o /tmp/tmps08cgrjr/a.out
2025-03-21 14:03:41,557 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpw2ufy9ss/test.o -laio -o /tmp/tmpw2ufy9ss/a.out
2025-03-21 14:03:41,902 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:03:41,902 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-03-41_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:03:41,902 - INFO - Starting training pipeline
2025-03-21 14:03:41,902 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:03:42,002 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:03:42,002 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-03-41_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:03:42,002 - INFO - Starting training pipeline
2025-03-21 14:03:42,002 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:03:42,179 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:03:42,179 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:03:42,267 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:03:42,268 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:03:44,130 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:03:47,260 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:03:47,946 - INFO - Loading datasets from 1 sources
2025-03-21 14:03:47,950 - INFO - Loading datasets from 1 sources
2025-03-21 14:03:47,952 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:03:47,953 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:03:47,953 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:03:47,954 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:03:47,955 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:03:47,955 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:03:49,042 - INFO - Created 32242 CPO samples
2025-03-21 14:03:49,042 - INFO - Created 32242 CPO samples
2025-03-21 14:03:49,228 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:03:49,247 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:04:11,362 - INFO - Starting training
2025-03-21 14:04:33,271 - INFO - Starting training
2025-03-21 14:07:07,610 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmplilki7_r/test.c -o /tmp/tmplilki7_r/test.o
2025-03-21 14:07:07,631 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmplilki7_r/test.o -laio -o /tmp/tmplilki7_r/a.out
2025-03-21 14:07:07,643 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpm8w5osqb/test.c -o /tmp/tmpm8w5osqb/test.o
2025-03-21 14:07:07,659 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpm8w5osqb/test.o -laio -o /tmp/tmpm8w5osqb/a.out
2025-03-21 14:07:07,995 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:07:07,995 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-07-07_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:07:07,995 - INFO - Starting training pipeline
2025-03-21 14:07:07,995 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:07:08,094 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:07:08,094 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-07-07_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:07:08,095 - INFO - Starting training pipeline
2025-03-21 14:07:08,095 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:07:08,273 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:07:08,273 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:07:08,361 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:07:08,361 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:07:10,267 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:07:13,360 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:07:14,055 - INFO - Loading datasets from 1 sources
2025-03-21 14:07:14,058 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:07:14,059 - INFO - Loading datasets from 1 sources
2025-03-21 14:07:14,060 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:07:14,060 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:07:14,065 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:07:14,066 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:07:14,066 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:07:15,170 - INFO - Created 32242 CPO samples
2025-03-21 14:07:15,172 - INFO - Created 32242 CPO samples
2025-03-21 14:07:15,358 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:07:15,368 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:07:37,113 - INFO - Starting training
2025-03-21 14:07:58,939 - INFO - Starting training
2025-03-21 14:09:39,226 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpog20fzf2/test.c -o /tmp/tmpog20fzf2/test.o
2025-03-21 14:09:39,248 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpog20fzf2/test.o -laio -o /tmp/tmpog20fzf2/a.out
2025-03-21 14:09:39,570 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:09:39,571 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-09-39_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:09:39,571 - INFO - Starting training pipeline
2025-03-21 14:09:39,571 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:09:39,860 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:09:39,860 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:09:43,104 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:09:43,478 - INFO - Loading datasets from 1 sources
2025-03-21 14:09:43,484 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:09:43,486 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:09:43,486 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:09:44,566 - INFO - Created 32242 CPO samples
2025-03-21 14:09:44,759 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:10:06,430 - INFO - Starting training
2025-03-21 14:10:16,435 - ERROR - Training failed: CUDA out of memory. Tried to allocate 3.21 GiB. GPU 0 has a total capacity of 47.43 GiB of which 3.08 GiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 43.28 GiB is allocated by PyTorch, and 383.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 475, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1955, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = ds_initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/__init__.py", line 181, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 1293, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 1617, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 361, in __init__
    self._setup_for_real_optimizer()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 471, in _setup_for_real_optimizer
    self._create_fp32_partitions()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 867, in _create_fp32_partitions
    self.device).clone().float().detach())
                         ^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.21 GiB. GPU 0 has a total capacity of 47.43 GiB of which 3.08 GiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 43.28 GiB is allocated by PyTorch, and 383.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-21 14:10:16,437 - ERROR - Training pipeline failed: CUDA out of memory. Tried to allocate 3.21 GiB. GPU 0 has a total capacity of 47.43 GiB of which 3.08 GiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 43.28 GiB is allocated by PyTorch, and 383.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 513, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 475, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1955, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = ds_initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/__init__.py", line 181, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 1293, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 1617, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 361, in __init__
    self._setup_for_real_optimizer()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 471, in _setup_for_real_optimizer
    self._create_fp32_partitions()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 867, in _create_fp32_partitions
    self.device).clone().float().detach())
                         ^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.21 GiB. GPU 0 has a total capacity of 47.43 GiB of which 3.08 GiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 43.28 GiB is allocated by PyTorch, and 383.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-21 14:13:33,477 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpj4z_ajr9/test.c -o /tmp/tmpj4z_ajr9/test.o
2025-03-21 14:13:33,485 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpumldek9r/test.c -o /tmp/tmpumldek9r/test.o
2025-03-21 14:13:33,495 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpj4z_ajr9/test.o -laio -o /tmp/tmpj4z_ajr9/a.out
2025-03-21 14:13:33,501 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpumldek9r/test.o -laio -o /tmp/tmpumldek9r/a.out
2025-03-21 14:13:33,856 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:13:33,856 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-13-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:13:33,856 - INFO - Starting training pipeline
2025-03-21 14:13:33,856 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:13:33,951 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:13:33,951 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-13-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:13:33,951 - INFO - Starting training pipeline
2025-03-21 14:13:33,951 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:13:34,137 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:13:34,137 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:13:34,216 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:13:34,217 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:13:36,086 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:13:39,242 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:13:40,234 - INFO - Loading datasets from 1 sources
2025-03-21 14:13:40,238 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:13:40,239 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:13:40,239 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:13:40,239 - INFO - Loading datasets from 1 sources
2025-03-21 14:13:40,242 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:13:40,243 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:13:40,243 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:13:41,343 - INFO - Created 32242 CPO samples
2025-03-21 14:13:41,369 - INFO - Created 32242 CPO samples
2025-03-21 14:13:41,541 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:13:41,566 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:14:03,330 - INFO - Starting training
2025-03-21 14:14:24,983 - INFO - Starting training
2025-03-21 14:22:32,039 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp8cgacv6o/test.c -o /tmp/tmp8cgacv6o/test.o
2025-03-21 14:22:32,042 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpbg_ymrml/test.c -o /tmp/tmpbg_ymrml/test.o
2025-03-21 14:22:32,059 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp8cgacv6o/test.o -laio -o /tmp/tmp8cgacv6o/a.out
2025-03-21 14:22:32,061 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpbg_ymrml/test.o -laio -o /tmp/tmpbg_ymrml/a.out
2025-03-21 14:22:32,414 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:22:32,415 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-22-31_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:22:32,415 - INFO - Starting training pipeline
2025-03-21 14:22:32,415 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:22:32,512 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:22:32,512 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-22-31_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:22:32,513 - INFO - Starting training pipeline
2025-03-21 14:22:32,513 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:22:32,691 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:22:32,691 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:22:32,779 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:22:32,779 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:22:34,620 - INFO - Model moved to cuda:1
2025-03-21 14:22:34,646 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:22:37,737 - INFO - Model moved to cuda:0
2025-03-21 14:22:37,758 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:22:38,448 - INFO - Loading datasets from 1 sources
2025-03-21 14:22:38,452 - INFO - Loading datasets from 1 sources
2025-03-21 14:22:38,453 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:22:38,455 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:22:38,455 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:22:38,456 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:22:38,457 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:22:38,457 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:22:39,568 - INFO - Created 32242 CPO samples
2025-03-21 14:22:39,573 - INFO - Created 32242 CPO samples
2025-03-21 14:22:39,760 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:22:39,761 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:23:02,086 - INFO - Starting training
2025-03-21 14:23:24,024 - INFO - Starting training
2025-03-21 14:26:51,604 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpe2xea3rt/test.c -o /tmp/tmpe2xea3rt/test.o
2025-03-21 14:26:51,606 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp75y5g8ss/test.c -o /tmp/tmp75y5g8ss/test.o
2025-03-21 14:26:51,627 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpe2xea3rt/test.o -laio -o /tmp/tmpe2xea3rt/a.out
2025-03-21 14:26:51,627 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp75y5g8ss/test.o -laio -o /tmp/tmp75y5g8ss/a.out
2025-03-21 14:26:51,978 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:26:51,979 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-26-51_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:26:51,979 - INFO - Starting training pipeline
2025-03-21 14:26:51,979 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:26:52,068 - INFO - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:26:52,069 - INFO - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_14-26-51_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:26:52,069 - INFO - Starting training pipeline
2025-03-21 14:26:52,069 - INFO - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:26:52,260 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:26:52,260 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:26:52,333 - INFO - Added special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:26:52,333 - INFO - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 14:26:54,183 - INFO - Model moved to cuda:1
2025-03-21 14:26:54,209 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:26:57,308 - INFO - Model moved to cuda:0
2025-03-21 14:26:57,329 - INFO - Resizing token embeddings from 0 to 128259
2025-03-21 14:26:58,023 - INFO - Loading datasets from 1 sources
2025-03-21 14:26:58,027 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:26:58,027 - INFO - Loading datasets from 1 sources
2025-03-21 14:26:58,028 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:26:58,028 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:26:58,030 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:26:58,031 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:26:58,032 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:26:59,120 - INFO - Created 32242 CPO samples
2025-03-21 14:26:59,133 - INFO - Created 32242 CPO samples
2025-03-21 14:26:59,322 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:26:59,332 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 14:27:21,001 - INFO - Starting training
2025-03-21 14:27:43,149 - INFO - Starting training
2025-03-21 14:29:10,263 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp8mhd5c1e/test.c -o /tmp/tmp8mhd5c1e/test.o
2025-03-21 14:29:10,281 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpf10q0fes/test.c -o /tmp/tmpf10q0fes/test.o
2025-03-21 14:29:10,284 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp8mhd5c1e/test.o -laio -o /tmp/tmp8mhd5c1e/a.out
2025-03-21 14:29:10,297 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpf10q0fes/test.o -laio -o /tmp/tmpf10q0fes/a.out
2025-03-21 14:29:10,651 - INFO - Starting training pipeline
2025-03-21 14:29:10,652 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:29:10,748 - INFO - Starting training pipeline
2025-03-21 14:29:10,748 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:29:11,447 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:29:11,447 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:29:11,993 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:29:11,993 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:29:13,271 - INFO - Model moved to cuda:0
2025-03-21 14:29:13,272 - INFO - Resizing token embeddings from 0 to 151648
2025-03-21 14:29:14,151 - INFO - Model moved to cuda:1
2025-03-21 14:29:14,151 - INFO - Resizing token embeddings from 0 to 151648
2025-03-21 14:29:14,164 - INFO - Loading datasets from 1 sources
2025-03-21 14:29:14,164 - INFO - Loading datasets from 1 sources
2025-03-21 14:29:14,170 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:29:14,170 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:29:14,172 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:29:14,172 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:29:14,172 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:29:14,172 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:29:15,203 - INFO - Created 32242 CPO samples
2025-03-21 14:29:15,206 - INFO - Created 32242 CPO samples
2025-03-21 14:29:15,387 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:29:15,406 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:29:59,716 - INFO - Checkpoint detected, resuming training at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.
2025-03-21 14:29:59,716 - INFO - Starting training
2025-03-21 14:30:43,699 - INFO - Checkpoint detected, resuming training at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.
2025-03-21 14:30:43,699 - INFO - Starting training
2025-03-21 14:30:45,119 - ERROR - Training failed: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 476, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2389, in _inner_training_loop
    deepspeed_load_checkpoint(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
    raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
ValueError: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-21 14:30:45,120 - ERROR - Training pipeline failed: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 510, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 476, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2389, in _inner_training_loop
    deepspeed_load_checkpoint(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
    raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
ValueError: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-21 14:30:45,281 - ERROR - Training failed: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 476, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2389, in _inner_training_loop
    deepspeed_load_checkpoint(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
    raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
ValueError: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-21 14:30:45,281 - ERROR - Training pipeline failed: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 510, in main
    train(script_args, training_args)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 476, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2389, in _inner_training_loop
    deepspeed_load_checkpoint(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
    raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
ValueError: Can't find a valid checkpoint at save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-21 14:31:41,929 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpl0ho0_ot/test.c -o /tmp/tmpl0ho0_ot/test.o
2025-03-21 14:31:41,935 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp0c39b7_o/test.c -o /tmp/tmp0c39b7_o/test.o
2025-03-21 14:31:41,950 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp0c39b7_o/test.o -laio -o /tmp/tmp0c39b7_o/a.out
2025-03-21 14:31:41,951 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpl0ho0_ot/test.o -laio -o /tmp/tmpl0ho0_ot/a.out
2025-03-21 14:31:42,312 - INFO - Starting training pipeline
2025-03-21 14:31:42,312 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:31:42,394 - INFO - Starting training pipeline
2025-03-21 14:31:42,395 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:31:42,971 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:31:42,971 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:31:43,103 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:31:43,103 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:31:46,307 - INFO - Model moved to cuda:1
2025-03-21 14:31:46,307 - INFO - Resizing token embeddings from 0 to 151648
2025-03-21 14:31:46,444 - INFO - Model moved to cuda:0
2025-03-21 14:31:46,444 - INFO - Resizing token embeddings from 0 to 151648
2025-03-21 14:31:46,457 - INFO - Loading datasets from 1 sources
2025-03-21 14:31:46,458 - INFO - Loading datasets from 1 sources
2025-03-21 14:31:46,461 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:31:46,462 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:31:46,463 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:31:46,463 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:31:46,463 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:31:46,463 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:31:47,484 - INFO - Created 32242 CPO samples
2025-03-21 14:31:47,487 - INFO - Created 32242 CPO samples
2025-03-21 14:31:47,666 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:31:47,670 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:32:31,720 - INFO - Starting training
2025-03-21 14:33:16,283 - INFO - Starting training
2025-03-21 14:33:53,614 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp6j7iwa6b/test.c -o /tmp/tmp6j7iwa6b/test.o
2025-03-21 14:33:53,633 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp6j7iwa6b/test.o -laio -o /tmp/tmp6j7iwa6b/a.out
2025-03-21 14:33:53,958 - INFO - Starting training pipeline
2025-03-21 14:33:53,958 - INFO - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:33:54,609 - INFO - Added special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:33:54,609 - INFO - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:33:55,820 - INFO - Model moved to cuda:0
2025-03-21 14:33:55,821 - INFO - Resizing token embeddings from 0 to 151648
2025-03-21 14:33:55,824 - INFO - Loading datasets from 1 sources
2025-03-21 14:33:55,830 - INFO - Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:33:55,831 - INFO - Combined dataset contains 6373 examples
2025-03-21 14:33:55,831 - INFO - Preparing CPO dataset from 6373 examples
2025-03-21 14:33:56,839 - INFO - Created 32242 CPO samples
2025-03-21 14:33:57,036 - INFO - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:34:40,652 - INFO - Starting training
2025-03-21 14:38:04,481 - INFO - Rank 0: Distributed training initialized with 1 GPUs
2025-03-21 14:38:04,531 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpc30rl90j/test.c -o /tmp/tmpc30rl90j/test.o
2025-03-21 14:38:04,552 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpc30rl90j/test.o -laio -o /tmp/tmpc30rl90j/a.out
2025-03-21 14:38:04,872 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:38:04,872 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-38-04_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:38:04,873 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:38:04,873 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:38:04,873 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:38:05,514 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:38:05,514 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:38:07,942 - INFO - Rank 0: Model moved to cuda:0
2025-03-21 14:38:07,942 - INFO - Rank 0: Resizing token embeddings from 0 to 151648
2025-03-21 14:38:07,945 - INFO - Rank 0: Loading dataset
2025-03-21 14:38:07,946 - INFO - Rank 0: Loading datasets from 1 sources
2025-03-21 14:38:07,951 - INFO - Rank 0: Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:38:07,953 - INFO - Rank 0: Combined dataset contains 6373 examples
2025-03-21 14:38:07,953 - INFO - Rank 0: Preparing CPO dataset
2025-03-21 14:38:07,953 - INFO - Rank 0: Preparing CPO dataset from 6373 examples with 2 workers
2025-03-21 14:38:07,955 - ERROR - Rank 0: Training failed: maximum recursion depth exceeded
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 330, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 267, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, training_args.dataloader_num_workers, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 213, in prepare_cpo_dataset
    samples = prepare_cpo_batch(batch, tokenizer, args.max_neg_paths)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 199, in prepare_cpo_batch
    return prepare_cpo_batch(batch, tokenizer, max_neg_paths)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 199, in prepare_cpo_batch
    return prepare_cpo_batch(batch, tokenizer, max_neg_paths)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 199, in prepare_cpo_batch
    return prepare_cpo_batch(batch, tokenizer, max_neg_paths)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 993 more times]
RecursionError: maximum recursion depth exceeded
2025-03-21 14:39:47,500 - INFO - Rank 0: Distributed training initialized with 1 GPUs
2025-03-21 14:39:47,551 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpllvqvuqg/test.c -o /tmp/tmpllvqvuqg/test.o
2025-03-21 14:39:47,572 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpllvqvuqg/test.o -laio -o /tmp/tmpllvqvuqg/a.out
2025-03-21 14:39:47,885 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:39:47,885 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-39-47_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:39:47,886 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:39:47,886 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:39:47,886 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:39:48,471 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:39:48,471 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:39:49,618 - INFO - Rank 0: Model moved to cuda:0
2025-03-21 14:39:49,618 - INFO - Rank 0: Resizing token embeddings from 0 to 151648
2025-03-21 14:39:49,621 - INFO - Rank 0: Loading dataset
2025-03-21 14:39:49,621 - INFO - Rank 0: Loading datasets from 1 sources
2025-03-21 14:39:49,627 - INFO - Rank 0: Loaded dataset from data/processed/rmanluo/RoG-webqsp_train with 6373 examples
2025-03-21 14:39:49,629 - INFO - Rank 0: Combined dataset contains 6373 examples
2025-03-21 14:39:49,629 - INFO - Rank 0: Preparing CPO dataset
2025-03-21 14:39:49,629 - INFO - Rank 0: Preparing CPO dataset from 6373 examples with 2 workers
2025-03-21 14:39:50,655 - INFO - Rank 0: Created 32242 CPO samples
2025-03-21 14:39:50,849 - INFO - Rank 0: Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:39:50,884 - INFO - Rank 0: Trainer config created
2025-03-21 14:39:50,884 - INFO - Rank 0: Initializing trainer
2025-03-21 14:40:35,199 - INFO - Rank 0: Starting training loop
2025-03-21 14:41:08,789 - INFO - Rank 0: Distributed training initialized with 2 GPUs
2025-03-21 14:41:08,838 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp7tdgdgj5/test.c -o /tmp/tmp7tdgdgj5/test.o
2025-03-21 14:41:08,859 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp7tdgdgj5/test.o -laio -o /tmp/tmp7tdgdgj5/a.out
2025-03-21 14:41:08,922 - INFO - Rank 1: Distributed training initialized with 2 GPUs
2025-03-21 14:41:08,980 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp7pdqk_dx/test.c -o /tmp/tmp7pdqk_dx/test.o
2025-03-21 14:41:08,999 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp7pdqk_dx/test.o -laio -o /tmp/tmp7pdqk_dx/a.out
2025-03-21 14:41:09,187 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:41:09,187 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-41-08_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:41:09,188 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:41:09,188 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:41:09,188 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:41:09,325 - INFO - Rank 1: Starting training pipeline
2025-03-21 14:41:09,325 - ERROR - Rank 1: Training failed: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 404, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 329, in train
    os.makedirs(training_args.output_dir, exist_ok=(rank == 0))
  File "<frozen os>", line 225, in makedirs
FileExistsError: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
2025-03-21 14:41:09,782 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:41:09,783 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:44:23,987 - INFO - Rank 0: Distributed training initialized with 2 GPUs
2025-03-21 14:44:24,037 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpwc8du7gv/test.c -o /tmp/tmpwc8du7gv/test.o
2025-03-21 14:44:24,057 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpwc8du7gv/test.o -laio -o /tmp/tmpwc8du7gv/a.out
2025-03-21 14:44:24,137 - INFO - Rank 1: Distributed training initialized with 2 GPUs
2025-03-21 14:44:24,191 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpsf5cehnh/test.c -o /tmp/tmpsf5cehnh/test.o
2025-03-21 14:44:24,210 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpsf5cehnh/test.o -laio -o /tmp/tmpsf5cehnh/a.out
2025-03-21 14:44:24,383 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:44:24,383 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-44-23_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:44:24,384 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:44:24,384 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:44:24,384 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:44:24,538 - INFO - Rank 1: Starting training pipeline
2025-03-21 14:44:24,538 - ERROR - Rank 1: Training failed: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 404, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 329, in train
    os.makedirs(training_args.output_dir, exist_ok=(rank == 0))
  File "<frozen os>", line 225, in makedirs
FileExistsError: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
2025-03-21 14:44:25,011 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:44:25,011 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:44:46,069 - INFO - Rank 0: Distributed training initialized with 2 GPUs
2025-03-21 14:44:46,122 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpxoy0di2l/test.c -o /tmp/tmpxoy0di2l/test.o
2025-03-21 14:44:46,144 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpxoy0di2l/test.o -laio -o /tmp/tmpxoy0di2l/a.out
2025-03-21 14:44:46,193 - INFO - Rank 1: Distributed training initialized with 2 GPUs
2025-03-21 14:44:46,248 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpit34fm7g/test.c -o /tmp/tmpit34fm7g/test.o
2025-03-21 14:44:46,270 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpit34fm7g/test.o -laio -o /tmp/tmpit34fm7g/a.out
2025-03-21 14:44:46,468 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:44:46,469 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-44-46_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:44:46,469 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:44:46,470 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:44:46,470 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:44:46,600 - INFO - Rank 1: Starting training pipeline
2025-03-21 14:44:46,600 - ERROR - Rank 1: Training failed: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 404, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 329, in train
    os.makedirs(training_args.output_dir, exist_ok=(rank == 0))
  File "<frozen os>", line 225, in makedirs
FileExistsError: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
2025-03-21 14:44:47,085 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:44:47,085 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:45:11,362 - INFO - Rank 0: Distributed training initialized with 2 GPUs
2025-03-21 14:45:11,414 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprq6winym/test.c -o /tmp/tmprq6winym/test.o
2025-03-21 14:45:11,432 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprq6winym/test.o -laio -o /tmp/tmprq6winym/a.out
2025-03-21 14:45:11,516 - INFO - Rank 1: Distributed training initialized with 2 GPUs
2025-03-21 14:45:11,578 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmppu4lmioa/test.c -o /tmp/tmppu4lmioa/test.o
2025-03-21 14:45:11,598 - INFO - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmppu4lmioa/test.o -laio -o /tmp/tmppu4lmioa/a.out
2025-03-21 14:45:11,754 - INFO - Rank 0: Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:45:11,754 - INFO - Rank 0: Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=2,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-45-11_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:45:11,755 - INFO - Rank 0: Starting training pipeline
2025-03-21 14:45:11,755 - INFO - Rank 0: Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:45:11,755 - INFO - Rank 0: Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:45:11,929 - INFO - Rank 1: Starting training pipeline
2025-03-21 14:45:11,929 - ERROR - Rank 1: Training failed: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 404, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 329, in train
    os.makedirs(training_args.output_dir, exist_ok=(rank == 0))
  File "<frozen os>", line 225, in makedirs
FileExistsError: [Errno 17] File exists: 'save_models/a-Qwen2-0.5B-Instruct'
2025-03-21 14:45:12,376 - INFO - Rank 0: Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:45:12,376 - INFO - Rank 0: Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:48:14,378 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:48:14,428 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpl04w3hjy/test.c -o /tmp/tmpl04w3hjy/test.o
2025-03-21 14:48:14,446 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpl04w3hjy/test.o -laio -o /tmp/tmpl04w3hjy/a.out
2025-03-21 14:48:14,563 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:48:14,616 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpms_edpei/test.c -o /tmp/tmpms_edpei/test.o
2025-03-21 14:48:14,636 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpms_edpei/test.o -laio -o /tmp/tmpms_edpei/a.out
2025-03-21 14:51:52,598 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:51:52,653 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpljzz0c8f/test.c -o /tmp/tmpljzz0c8f/test.o
2025-03-21 14:51:52,670 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpljzz0c8f/test.o -laio -o /tmp/tmpljzz0c8f/a.out
2025-03-21 14:51:52,747 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:51:52,799 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp8zfxrcz1/test.c -o /tmp/tmp8zfxrcz1/test.o
2025-03-21 14:51:52,820 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp8zfxrcz1/test.o -laio -o /tmp/tmp8zfxrcz1/a.out
2025-03-21 14:53:02,450 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:53:02,502 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp0dzc9_fl/test.c -o /tmp/tmp0dzc9_fl/test.o
2025-03-21 14:53:02,521 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp0dzc9_fl/test.o -laio -o /tmp/tmp0dzc9_fl/a.out
2025-03-21 14:53:02,528 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:53:02,575 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp49lny3tc/test.c -o /tmp/tmp49lny3tc/test.o
2025-03-21 14:53:02,593 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp49lny3tc/test.o -laio -o /tmp/tmp49lny3tc/a.out
2025-03-21 14:53:11,126 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:53:11,178 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpfgp2okx_/test.c -o /tmp/tmpfgp2okx_/test.o
2025-03-21 14:53:11,201 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpfgp2okx_/test.o -laio -o /tmp/tmpfgp2okx_/a.out
2025-03-21 14:53:11,268 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:53:11,325 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpm_vwk3sc/test.c -o /tmp/tmpm_vwk3sc/test.o
2025-03-21 14:53:11,346 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpm_vwk3sc/test.o -laio -o /tmp/tmpm_vwk3sc/a.out
2025-03-21 14:56:13,945 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:56:14,000 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp9uvba_d1/test.c -o /tmp/tmp9uvba_d1/test.o
2025-03-21 14:56:14,018 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:56:14,024 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp9uvba_d1/test.o -laio -o /tmp/tmp9uvba_d1/a.out
2025-03-21 14:56:14,067 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpyooi63bw/test.c -o /tmp/tmpyooi63bw/test.o
2025-03-21 14:56:14,084 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpyooi63bw/test.o -laio -o /tmp/tmpyooi63bw/a.out
2025-03-21 14:57:33,582 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:57:33,632 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpz5lfrgfj/test.c -o /tmp/tmpz5lfrgfj/test.o
2025-03-21 14:57:33,655 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpz5lfrgfj/test.o -laio -o /tmp/tmpz5lfrgfj/a.out
2025-03-21 14:57:33,733 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:57:33,784 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpi8gleu0v/test.c -o /tmp/tmpi8gleu0v/test.o
2025-03-21 14:57:33,803 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpi8gleu0v/test.o -laio -o /tmp/tmpi8gleu0v/a.out
2025-03-21 14:57:33,984 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:57:33,984 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-57-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:57:33,985 - INFO - Rank:0 - Starting training pipeline
2025-03-21 14:57:33,985 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:57:34,134 - INFO - Rank:1 - Starting training pipeline
2025-03-21 14:57:34,376 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:57:34,376 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:57:35,068 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:57:35,078 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:57:35,078 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:57:36,614 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 14:57:36,614 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 14:57:36,873 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 14:57:36,873 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 14:57:36,990 - INFO - Rank:1 - Loading dataset
2025-03-21 14:57:36,990 - INFO - Rank:0 - Loading dataset
2025-03-21 14:57:36,990 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 14:57:36,990 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 14:57:36,995 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 14:57:36,996 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 14:57:36,996 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 14:57:36,996 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 14:57:36,996 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 14:57:36,996 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 14:57:38,019 - INFO - Rank:1 - Created 32242 CPO samples
2025-03-21 14:57:38,022 - INFO - Rank:0 - Created 32242 CPO samples
2025-03-21 14:57:38,167 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:57:38,199 - INFO - Rank:0 - Trainer config created
2025-03-21 14:57:38,199 - INFO - Rank:0 - Initializing trainer
2025-03-21 14:57:38,201 - INFO - Rank:1 - Initializing trainer
2025-03-21 14:58:23,944 - INFO - Rank:0 - Starting training loop
2025-03-21 14:58:24,158 - ERROR - Rank:0 - Training failed: No module named 'triton.ops'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 327, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 297, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1944, in _prepare_deepspeed
    optimizer = map_pytorch_optim_to_deepspeed(optimizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 71, in map_pytorch_optim_to_deepspeed
    import bitsandbytes.optim as bnb_opt
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/__init__.py", line 17, in <module>
    from .triton_based_modules import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'
2025-03-21 14:59:12,688 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 14:59:12,745 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpvq4v_5kh/test.c -o /tmp/tmpvq4v_5kh/test.o
2025-03-21 14:59:12,766 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpvq4v_5kh/test.o -laio -o /tmp/tmpvq4v_5kh/a.out
2025-03-21 14:59:12,771 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 14:59:12,824 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpgtxec0tf/test.c -o /tmp/tmpgtxec0tf/test.o
2025-03-21 14:59:12,841 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpgtxec0tf/test.o -laio -o /tmp/tmpgtxec0tf/a.out
2025-03-21 14:59:13,097 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 14:59:13,097 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_14-59-12_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_prompt_length=512,
max_seq_length=1024,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
ooutput_dir=saved_models/llama3_align,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 14:59:13,099 - INFO - Rank:0 - Starting training pipeline
2025-03-21 14:59:13,099 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 14:59:13,214 - INFO - Rank:1 - Starting training pipeline
2025-03-21 14:59:13,483 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:59:13,483 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:59:14,108 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:59:14,121 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 14:59:14,122 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 14:59:15,570 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 14:59:15,570 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 14:59:15,604 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 14:59:15,604 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 14:59:15,740 - INFO - Rank:0 - Loading dataset
2025-03-21 14:59:15,740 - INFO - Rank:1 - Loading dataset
2025-03-21 14:59:15,740 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 14:59:15,740 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 14:59:15,745 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 14:59:15,746 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 14:59:15,746 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 14:59:15,746 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 14:59:15,746 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 14:59:15,746 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 14:59:16,774 - INFO - Rank:1 - Created 32242 CPO samples
2025-03-21 14:59:16,777 - INFO - Rank:0 - Created 32242 CPO samples
2025-03-21 14:59:16,921 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 14:59:16,960 - INFO - Rank:1 - Initializing trainer
2025-03-21 14:59:16,961 - INFO - Rank:0 - Trainer config created
2025-03-21 14:59:16,961 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:00:03,375 - INFO - Rank:0 - Starting training loop
2025-03-21 15:00:03,566 - ERROR - Rank:0 - Training failed: No module named 'triton.ops'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 336, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 306, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1944, in _prepare_deepspeed
    optimizer = map_pytorch_optim_to_deepspeed(optimizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 71, in map_pytorch_optim_to_deepspeed
    import bitsandbytes.optim as bnb_opt
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/__init__.py", line 17, in <module>
    from .triton_based_modules import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'
2025-03-21 15:04:43,642 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:04:43,695 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpiqmjl5xo/test.c -o /tmp/tmpiqmjl5xo/test.o
2025-03-21 15:04:43,717 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpiqmjl5xo/test.o -laio -o /tmp/tmpiqmjl5xo/a.out
2025-03-21 15:04:43,816 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:04:43,874 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpofhua135/test.c -o /tmp/tmpofhua135/test.o
2025-03-21 15:04:43,892 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpofhua135/test.o -laio -o /tmp/tmpofhua135/a.out
2025-03-21 15:04:44,040 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, n_path_per_sample=10, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', response_template='[/INST]')
2025-03-21 15:04:44,040 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-04-43_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:04:44,041 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:04:44,041 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:04:44,215 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:04:44,433 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:04:44,433 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:04:45,007 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:04:45,007 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:04:45,305 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:04:46,656 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:04:46,656 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:04:46,675 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:04:46,675 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:04:46,794 - INFO - Rank:0 - Loading dataset
2025-03-21 15:04:46,794 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:04:46,794 - INFO - Rank:1 - Loading dataset
2025-03-21 15:04:46,795 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:04:46,799 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:04:46,799 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:04:46,800 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:04:46,800 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:04:46,800 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:04:46,800 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:04:46,800 - ERROR - Rank:0 - Training failed: 'ScriptArguments' object has no attribute 'batch_size'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 323, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 266, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, training_args.dataloader_num_workers, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 206, in prepare_cpo_dataset
    batch_size = min(args.batch_size, len(dataset))
                     ^^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'batch_size'
2025-03-21 15:04:46,800 - ERROR - Rank:1 - Training failed: 'ScriptArguments' object has no attribute 'batch_size'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 323, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 266, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, training_args.dataloader_num_workers, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 206, in prepare_cpo_dataset
    batch_size = min(args.batch_size, len(dataset))
                     ^^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'batch_size'
2025-03-21 15:11:33,635 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:11:33,693 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp6fwendoc/test.c -o /tmp/tmp6fwendoc/test.o
2025-03-21 15:11:33,716 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp6fwendoc/test.o -laio -o /tmp/tmp6fwendoc/a.out
2025-03-21 15:11:33,787 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:11:33,841 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpe0dt0ayc/test.c -o /tmp/tmpe0dt0ayc/test.o
2025-03-21 15:11:33,864 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpe0dt0ayc/test.o -laio -o /tmp/tmpe0dt0ayc/a.out
2025-03-21 15:11:34,034 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, n_path_per_sample=10, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', response_template='[/INST]', batch_size=1024)
2025-03-21 15:11:34,034 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-11-33_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:11:34,036 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:11:34,036 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:11:34,198 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:11:34,409 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:11:34,409 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:11:35,026 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:11:35,026 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:11:35,036 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:11:36,403 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:11:36,404 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:11:36,416 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:11:36,416 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:11:36,532 - INFO - Rank:0 - Loading dataset
2025-03-21 15:11:36,532 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:11:36,532 - INFO - Rank:1 - Loading dataset
2025-03-21 15:11:36,533 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:11:36,538 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:11:36,540 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:11:36,540 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:11:36,540 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:11:36,541 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:11:36,541 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:11:36,542 - ERROR - Rank:0 - Training failed: 'ScriptArguments' object has no attribute 'max_neg_paths'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 324, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 267, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, training_args.dataloader_num_workers, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 212, in prepare_cpo_dataset
    samples = prepare_cpo_batch(batch, tokenizer, args.max_neg_paths)
                                                  ^^^^^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'max_neg_paths'
2025-03-21 15:11:36,542 - ERROR - Rank:1 - Training failed: 'ScriptArguments' object has no attribute 'max_neg_paths'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 324, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 267, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, training_args.dataloader_num_workers, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 212, in prepare_cpo_dataset
    samples = prepare_cpo_batch(batch, tokenizer, args.max_neg_paths)
                                                  ^^^^^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'max_neg_paths'
2025-03-21 15:12:58,455 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:12:58,507 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpcebm7any/test.c -o /tmp/tmpcebm7any/test.o
2025-03-21 15:12:58,529 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpcebm7any/test.o -laio -o /tmp/tmpcebm7any/a.out
2025-03-21 15:12:58,556 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:12:58,605 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpct7akslc/test.c -o /tmp/tmpct7akslc/test.o
2025-03-21 15:12:58,625 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpct7akslc/test.o -laio -o /tmp/tmpct7akslc/a.out
2025-03-21 15:12:58,857 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, n_path_per_sample=10, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', response_template='[/INST]', batch_size=1024)
2025-03-21 15:12:58,858 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-12-58_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:12:58,859 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:12:58,859 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:12:58,966 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:12:59,199 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:12:59,199 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:12:59,777 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:12:59,778 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:12:59,780 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:13:10,959 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:13:10,959 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:13:11,152 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:13:11,152 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:13:11,265 - INFO - Rank:0 - Loading dataset
2025-03-21 15:13:11,266 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:13:11,266 - INFO - Rank:1 - Loading dataset
2025-03-21 15:13:11,266 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:13:11,272 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:13:11,273 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:13:11,273 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:13:11,273 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:13:11,273 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:13:11,274 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:13:12,315 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:13:12,332 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:13:12,532 - ERROR - Rank:1 - Training failed: 'TrainingConfig' object has no attribute 'max_prompt_length'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 324, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 273, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 222, in create_trainer_config
    max_prompt_length=training_args.max_prompt_length,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TrainingConfig' object has no attribute 'max_prompt_length'
2025-03-21 15:13:12,546 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:13:12,547 - ERROR - Rank:0 - Training failed: 'TrainingConfig' object has no attribute 'max_prompt_length'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 324, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 273, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 222, in create_trainer_config
    max_prompt_length=training_args.max_prompt_length,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TrainingConfig' object has no attribute 'max_prompt_length'
2025-03-21 15:13:35,810 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:13:35,859 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpaoztava8/test.c -o /tmp/tmpaoztava8/test.o
2025-03-21 15:13:35,881 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpaoztava8/test.o -laio -o /tmp/tmpaoztava8/a.out
2025-03-21 15:13:35,938 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:13:35,988 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpsml3q9dy/test.c -o /tmp/tmpsml3q9dy/test.o
2025-03-21 15:13:36,008 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpsml3q9dy/test.o -laio -o /tmp/tmpsml3q9dy/a.out
2025-03-21 15:13:36,206 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, n_path_per_sample=10, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', response_template='[/INST]', batch_size=1024)
2025-03-21 15:13:36,206 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-13-35_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:13:36,207 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:13:36,207 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:13:36,334 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:13:36,558 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:13:36,558 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:13:37,135 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:13:37,150 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:13:37,151 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:13:38,495 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:13:38,495 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:13:38,537 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:13:38,537 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:13:38,652 - INFO - Rank:0 - Loading dataset
2025-03-21 15:13:38,652 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:13:38,652 - INFO - Rank:1 - Loading dataset
2025-03-21 15:13:38,653 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:13:38,658 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:13:38,658 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:13:38,658 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:13:38,658 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:13:38,658 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:13:38,658 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:13:39,698 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:13:39,709 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:13:39,913 - ERROR - Rank:1 - Training failed: 'ScriptArguments' object has no attribute 'beta'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 323, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 272, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 232, in create_trainer_config
    beta=args.beta,
         ^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'beta'
2025-03-21 15:13:39,926 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:13:39,926 - ERROR - Rank:0 - Training failed: 'ScriptArguments' object has no attribute 'beta'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 323, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 272, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 232, in create_trainer_config
    beta=args.beta,
         ^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'beta'
2025-03-21 15:14:01,211 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:14:01,262 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpad9qbngo/test.c -o /tmp/tmpad9qbngo/test.o
2025-03-21 15:14:01,283 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpad9qbngo/test.o -laio -o /tmp/tmpad9qbngo/a.out
2025-03-21 15:14:01,294 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:14:01,343 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmphczfwykm/test.c -o /tmp/tmphczfwykm/test.o
2025-03-21 15:14:01,362 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmphczfwykm/test.o -laio -o /tmp/tmphczfwykm/a.out
2025-03-21 15:14:01,615 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, n_path_per_sample=10, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', response_template='[/INST]', batch_size=1024)
2025-03-21 15:14:01,615 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-14-01_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:14:01,616 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:14:01,617 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:14:01,712 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:14:01,962 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:14:01,962 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:14:02,612 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:14:02,612 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:14:02,647 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:14:04,078 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:14:04,078 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:14:04,092 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:14:04,093 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:14:04,210 - INFO - Rank:0 - Loading dataset
2025-03-21 15:14:04,210 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:14:04,210 - INFO - Rank:1 - Loading dataset
2025-03-21 15:14:04,211 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:14:04,216 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:14:04,218 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:14:04,219 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:14:04,219 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:14:04,219 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:14:04,219 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:14:05,281 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:14:05,285 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:14:05,499 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:14:05,499 - ERROR - Rank:0 - Training failed: 'ScriptArguments' object has no attribute 'loss_type'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 322, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 271, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 232, in create_trainer_config
    loss_type=args.loss_type,
              ^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'loss_type'
2025-03-21 15:14:05,506 - ERROR - Rank:1 - Training failed: 'ScriptArguments' object has no attribute 'loss_type'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 322, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 271, in train
    trainer_config = create_trainer_config(args, training_args, rank)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 232, in create_trainer_config
    loss_type=args.loss_type,
              ^^^^^^^^^^^^^^
AttributeError: 'ScriptArguments' object has no attribute 'loss_type'
2025-03-21 15:16:01,913 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:16:01,964 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpso89u8te/test.c -o /tmp/tmpso89u8te/test.o
2025-03-21 15:16:01,981 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpso89u8te/test.o -laio -o /tmp/tmpso89u8te/a.out
2025-03-21 15:16:02,015 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:16:02,065 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmprnef1oah/test.c -o /tmp/tmprnef1oah/test.o
2025-03-21 15:16:02,081 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmprnef1oah/test.o -laio -o /tmp/tmprnef1oah/a.out
2025-03-21 15:16:02,306 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:16:02,306 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-16-01_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:16:02,307 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:16:02,308 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:16:02,409 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:16:02,640 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:16:02,640 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:16:03,266 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:16:05,932 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:16:05,933 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:16:07,322 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:16:07,322 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:16:07,329 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:16:07,329 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:16:07,447 - INFO - Rank:0 - Loading dataset
2025-03-21 15:16:07,447 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:16:07,447 - INFO - Rank:1 - Loading dataset
2025-03-21 15:16:07,447 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:16:07,452 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:16:07,452 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:16:07,452 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:16:07,452 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:16:07,452 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:16:07,452 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:16:08,555 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:16:08,589 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:16:08,801 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:16:08,806 - INFO - Rank:1 - Initializing trainer
2025-03-21 15:16:08,836 - INFO - Rank:0 - Trainer config created
2025-03-21 15:16:08,836 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:17:06,736 - INFO - Rank:0 - Starting training loop
2025-03-21 15:17:06,904 - ERROR - Rank:0 - Training failed: No module named 'triton.ops'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 325, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 295, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1944, in _prepare_deepspeed
    optimizer = map_pytorch_optim_to_deepspeed(optimizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 71, in map_pytorch_optim_to_deepspeed
    import bitsandbytes.optim as bnb_opt
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/__init__.py", line 17, in <module>
    from .triton_based_modules import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'
2025-03-21 15:20:53,119 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:20:53,171 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp_fsctte3/test.c -o /tmp/tmp_fsctte3/test.o
2025-03-21 15:20:53,189 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp_fsctte3/test.o -laio -o /tmp/tmp_fsctte3/a.out
2025-03-21 15:20:53,246 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:20:53,297 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpvgvow48q/test.c -o /tmp/tmpvgvow48q/test.o
2025-03-21 15:20:53,320 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpvgvow48q/test.o -laio -o /tmp/tmpvgvow48q/a.out
2025-03-21 15:20:53,508 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:20:53,509 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-20-53_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:20:53,510 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:20:53,510 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:20:53,650 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:20:53,862 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:20:53,862 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:20:54,487 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:20:54,514 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:20:54,515 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:20:55,944 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:20:55,944 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:20:55,961 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:20:55,961 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:20:56,077 - INFO - Rank:0 - Loading dataset
2025-03-21 15:20:56,077 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:20:56,078 - INFO - Rank:1 - Loading dataset
2025-03-21 15:20:56,078 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:20:56,083 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:20:56,085 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:20:56,085 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:20:56,085 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:20:56,085 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:20:56,085 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:20:57,214 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:20:57,214 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:20:57,426 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:20:57,461 - INFO - Rank:0 - Trainer config created
2025-03-21 15:20:57,461 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:20:57,465 - INFO - Rank:1 - Initializing trainer
2025-03-21 15:21:54,637 - INFO - Rank:0 - Starting training loop
2025-03-21 15:21:54,803 - ERROR - Rank:0 - Training failed: No module named 'triton.ops'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 325, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 295, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1394, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1944, in _prepare_deepspeed
    optimizer = map_pytorch_optim_to_deepspeed(optimizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 71, in map_pytorch_optim_to_deepspeed
    import bitsandbytes.optim as bnb_opt
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/__init__.py", line 17, in <module>
    from .triton_based_modules import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'
2025-03-21 15:23:53,643 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:23:53,693 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpcyvacs0v/test.c -o /tmp/tmpcyvacs0v/test.o
2025-03-21 15:23:53,711 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpcyvacs0v/test.o -laio -o /tmp/tmpcyvacs0v/a.out
2025-03-21 15:23:53,796 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:23:53,851 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpysvncf4n/test.c -o /tmp/tmpysvncf4n/test.o
2025-03-21 15:23:53,869 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpysvncf4n/test.o -laio -o /tmp/tmpysvncf4n/a.out
2025-03-21 15:23:53,895 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:23:53,895 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-23-53_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:23:53,896 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:23:53,896 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:23:54,046 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:23:54,255 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:23:54,256 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:23:54,856 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:23:54,857 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:23:54,884 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:23:56,471 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:23:56,471 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-21 15:23:56,481 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:23:56,481 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-21 15:23:56,610 - INFO - Rank:0 - Loading dataset
2025-03-21 15:23:56,611 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:23:56,611 - INFO - Rank:1 - Loading dataset
2025-03-21 15:23:56,611 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:23:56,615 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:23:56,618 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:23:56,618 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:23:56,618 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:23:56,618 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:23:56,618 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:23:57,736 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:23:57,741 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:23:57,949 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:23:57,982 - INFO - Rank:0 - Trainer config created
2025-03-21 15:23:57,982 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:23:57,983 - INFO - Rank:1 - Initializing trainer
2025-03-21 15:24:56,163 - INFO - Rank:0 - Starting training loop
2025-03-21 15:25:53,917 - INFO - Rank:1 - Starting training loop
2025-03-21 15:32:46,224 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-21 15:32:46,278 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpagg5_sjr/test.c -o /tmp/tmpagg5_sjr/test.o
2025-03-21 15:32:46,299 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpagg5_sjr/test.o -laio -o /tmp/tmpagg5_sjr/a.out
2025-03-21 15:32:46,325 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-21 15:32:46,376 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpekx0kzk1/test.c -o /tmp/tmpekx0kzk1/test.o
2025-03-21 15:32:46,397 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpekx0kzk1/test.o -laio -o /tmp/tmpekx0kzk1/a.out
2025-03-21 15:32:46,473 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:32:46,474 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-32-46_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:32:46,476 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:32:46,476 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:32:46,589 - INFO - Rank:1 - Starting training pipeline
2025-03-21 15:32:46,815 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:32:46,815 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:32:47,447 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:32:47,465 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:32:47,465 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:32:48,344 - INFO - Rank:1 - Model moved to cuda:1
2025-03-21 15:32:48,344 - INFO - Rank:1 - Resizing token embeddings from 151936 to 151646
2025-03-21 15:32:48,346 - INFO - Rank:1 - Loading dataset
2025-03-21 15:32:48,347 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-21 15:32:48,352 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-21 15:32:48,399 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:32:48,399 - INFO - Rank:0 - Resizing token embeddings from 151936 to 151648
2025-03-21 15:32:48,401 - INFO - Rank:0 - Loading dataset
2025-03-21 15:32:48,401 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:32:48,406 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:32:48,406 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:32:48,406 - INFO - Rank:1 - Preparing CPO dataset
2025-03-21 15:32:48,406 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:32:48,406 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:32:49,535 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:32:49,544 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-21 15:32:49,752 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:32:49,784 - INFO - Rank:0 - Trainer config created
2025-03-21 15:32:49,784 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:32:49,791 - INFO - Rank:1 - Initializing trainer
2025-03-21 15:33:47,815 - INFO - Rank:0 - Starting training loop
2025-03-21 15:34:45,565 - INFO - Rank:1 - Starting training loop
2025-03-21 15:42:39,381 - INFO - Rank:0 - Distributed training initialized with 1 GPUs
2025-03-21 15:42:39,429 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp0x5wd3y5/test.c -o /tmp/tmp0x5wd3y5/test.o
2025-03-21 15:42:39,455 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp0x5wd3y5/test.o -laio -o /tmp/tmp0x5wd3y5/a.out
2025-03-21 15:42:39,636 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:42:39,636 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-42-39_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:42:39,637 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:42:39,637 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:42:39,896 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:42:40,553 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:42:40,553 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:42:41,437 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:42:41,438 - INFO - Rank:0 - Resizing token embeddings from 151936 to 151648
2025-03-21 15:42:41,439 - INFO - Rank:0 - Loading dataset
2025-03-21 15:42:41,440 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:42:41,446 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:42:41,447 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:42:41,447 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:42:42,555 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:42:42,764 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|im_end|>\n', 'rejected': '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|im_end|>\n<|im_start|>assistant\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|im_end|>\n'}
2025-03-21 15:42:42,800 - INFO - Rank:0 - Trainer config created
2025-03-21 15:42:42,800 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:43:39,599 - INFO - Rank:0 - Starting training loop
2025-03-21 15:44:45,826 - INFO - Rank:0 - Distributed training initialized with 1 GPUs
2025-03-21 15:44:45,876 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpsgr23rl2/test.c -o /tmp/tmpsgr23rl2/test.o
2025-03-21 15:44:45,896 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpsgr23rl2/test.o -laio -o /tmp/tmpsgr23rl2/a.out
2025-03-21 15:44:46,083 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=True, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:44:46,083 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=True,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-Qwen2-0.5B-Instruct/runs/Mar21_15-44-45_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:44:46,084 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:44:46,084 - INFO - Rank:0 - Output directory prepared: save_models/a-Qwen2-0.5B-Instruct
2025-03-21 15:44:46,357 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:44:47,068 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:44:47,069 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-21 15:44:58,982 - INFO - Rank:0 - Distributed training initialized with 1 GPUs
2025-03-21 15:44:59,030 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpwl_h10dt/test.c -o /tmp/tmpwl_h10dt/test.o
2025-03-21 15:44:59,050 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpwl_h10dt/test.o -laio -o /tmp/tmpwl_h10dt/a.out
2025-03-21 15:44:59,231 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=True, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1, batch_size=100)
2025-03-21 15:44:59,231 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=True,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/a-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar21_15-44-58_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/a-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=a-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-21 15:44:59,232 - INFO - Rank:0 - Starting training pipeline
2025-03-21 15:44:59,232 - INFO - Rank:0 - Output directory prepared: save_models/a-0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 15:44:59,509 - INFO - Rank:0 - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 15:44:59,767 - INFO - Rank:0 - Added 3 special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-21 15:44:59,768 - INFO - Rank:0 - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-21 15:45:02,599 - INFO - Rank:0 - Model moved to cuda:0
2025-03-21 15:45:02,621 - INFO - Rank:0 - Resizing token embeddings from 128256 to 128259
2025-03-21 15:45:02,969 - INFO - Rank:0 - Initializing LoRA adapter
2025-03-21 15:45:03,078 - INFO - Rank:0 - LoRA adapter initialized with rank 8
2025-03-21 15:45:03,078 - INFO - Rank:0 - Loading dataset
2025-03-21 15:45:03,078 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-21 15:45:03,085 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-21 15:45:03,086 - INFO - Rank:0 - Preparing CPO dataset
2025-03-21 15:45:03,086 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples with 1 workers
2025-03-21 15:45:04,298 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-21 15:45:04,571 - INFO - Rank:0 - Sample training example: {'prompt': 'Question: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.\nCompare paths:', 'chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.sibling_s]->m.0gxnnwp ; m.0gxnnwp-[people.sibling_relationship.sibling]->Jaxon Bieber</PATH>\nAnalysis: High semantic relevance to question intent\nAnswer: Jaxon Bieber<|eot_id|>', 'rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: what is the name of justin bieber brother\nEntity: Justin Bieber\nEvaluate the reasoning path for accuracy and relevance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nPath: <PATH>Justin Bieber-[people.person.education]->m.0yrlqp1</PATH>\nAnalysis: Low relevance, does not address question intent\nAnswer: Cannot determine<|eot_id|>'}
2025-03-21 15:45:04,608 - INFO - Rank:0 - Trainer config created
2025-03-21 15:45:04,609 - INFO - Rank:0 - Initializing trainer
2025-03-21 15:45:04,711 - INFO - Rank:0 - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
2025-03-21 15:45:34,332 - INFO - Rank:0 - Starting training loop
2025-03-21 23:57:16,788 - ERROR - Rank:0 - Training failed: [Errno 5] Input/output error
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 325, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 295, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2612, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 3081, in _maybe_log_save_evaluate
    self.log(logs, start_time)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 998, in log
    return super().log(logs, start_time)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 3614, in log
    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer_callback.py", line 550, in on_log
    return self.call_event("on_log", args, state, control, logs=logs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer_callback.py", line 557, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer_callback.py", line 694, in on_log
    self.training_bar.write(str(shallow_logs))
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/tqdm/std.py", line 723, in write
    fp.write(end)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/wandb/sdk/lib/redirect.py", line 645, in write
    self._old_write(data)
OSError: [Errno 5] Input/output error
2025-03-22 14:58:57,928 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 14:58:58,018 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:01:12,250 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 15:01:12,335 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:06:06,267 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 15:06:06,397 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:08:33,398 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 15:08:33,513 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:11:37,593 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 15:11:37,640 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpg6p54_f6/test.c -o /tmp/tmpg6p54_f6/test.o
2025-03-22 15:11:37,661 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpg6p54_f6/test.o -laio -o /tmp/tmpg6p54_f6/a.out
2025-03-22 15:11:37,745 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:11:37,799 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpr5scdzbs/test.c -o /tmp/tmpr5scdzbs/test.o
2025-03-22 15:11:37,821 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpr5scdzbs/test.o -laio -o /tmp/tmpr5scdzbs/a.out
2025-03-22 15:11:57,969 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-22 15:11:58,016 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp1h69fghh/test.c -o /tmp/tmp1h69fghh/test.o
2025-03-22 15:11:58,033 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp1h69fghh/test.o -laio -o /tmp/tmp1h69fghh/a.out
2025-03-22 15:11:58,072 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-22 15:11:58,126 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpb2iae8b8/test.c -o /tmp/tmpb2iae8b8/test.o
2025-03-22 15:11:58,147 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpb2iae8b8/test.o -laio -o /tmp/tmpb2iae8b8/a.out
2025-03-22 15:11:58,201 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='/mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-22 15:11:58,201 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659/runs/Mar22_15-11-57_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-0e9e39f249a16976918f6564b8830bc894c89659,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-22 15:11:58,202 - INFO - Rank:0 - Starting training pipeline
2025-03-22 15:11:58,202 - INFO - Rank:0 - Output directory prepared: save_models/GCR-0e9e39f249a16976918f6564b8830bc894c89659
2025-03-22 15:11:58,321 - INFO - Rank:1 - Starting training pipeline
2025-03-22 15:11:58,539 - INFO - Rank:1 - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-22 15:11:58,539 - INFO - Rank:0 - Initializing tokenizer from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-22 15:11:58,815 - INFO - Rank:0 - Added 3 special tokens: {'pad_token': '<PAD>', 'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-22 15:11:58,815 - INFO - Rank:0 - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-22 15:11:58,817 - INFO - Rank:1 - Initializing model from /mnt/data/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659
2025-03-22 15:12:00,425 - INFO - Rank:1 - Model moved to cuda:1
2025-03-22 15:12:00,451 - INFO - Rank:1 - Resizing token embeddings from 0 to 128256
2025-03-22 15:12:03,555 - INFO - Rank:0 - Model moved to cuda:0
2025-03-22 15:12:03,576 - INFO - Rank:0 - Resizing token embeddings from 0 to 128259
2025-03-24 08:55:54,516 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 08:55:54,567 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpd6xo_c5f/test.c -o /tmp/tmpd6xo_c5f/test.o
2025-03-24 08:55:54,588 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpd6xo_c5f/test.o -laio -o /tmp/tmpd6xo_c5f/a.out
2025-03-24 08:55:54,599 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 08:55:54,649 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp5i6n9ffv/test.c -o /tmp/tmp5i6n9ffv/test.o
2025-03-24 08:55:54,668 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp5i6n9ffv/test.o -laio -o /tmp/tmp5i6n9ffv/a.out
2025-03-24 08:55:54,761 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 08:55:54,761 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_08-55-54_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 08:55:54,763 - INFO - Rank:0 - Starting training pipeline
2025-03-24 08:55:54,763 - INFO - Rank:0 - Output directory prepared: save_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 08:55:54,866 - INFO - Rank:1 - Starting training pipeline
2025-03-24 08:55:55,094 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:55:55,094 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:55:55,701 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:55:55,705 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 08:55:55,705 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:55:56,794 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 08:55:56,794 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-24 08:55:56,836 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 08:55:56,836 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-24 08:55:56,850 - INFO - Rank:0 - Loading dataset
2025-03-24 08:55:56,850 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 08:55:56,850 - INFO - Rank:1 - Loading dataset
2025-03-24 08:55:56,850 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 08:55:56,856 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 08:55:56,856 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 08:55:56,897 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 08:55:56,897 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 08:55:56,897 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 08:55:56,897 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 08:55:56,900 - ERROR - Rank:0 - Training failed: type object 'Template' has no attribute 'CPO_COMPARE_PROMPT'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 293, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 241, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 183, in prepare_cpo_dataset
    compare_prompt = f"{prompt}\n{Template.CPO_COMPARE_PROMPT}"
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'Template' has no attribute 'CPO_COMPARE_PROMPT'
2025-03-24 08:55:56,900 - ERROR - Rank:1 - Training failed: type object 'Template' has no attribute 'CPO_COMPARE_PROMPT'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 293, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 241, in train
    cpo_dataset = prepare_cpo_dataset(dataset, tokenizer, args, rank)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 183, in prepare_cpo_dataset
    compare_prompt = f"{prompt}\n{Template.CPO_COMPARE_PROMPT}"
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'Template' has no attribute 'CPO_COMPARE_PROMPT'
2025-03-24 08:59:02,229 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 08:59:02,283 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp4am0tehq/test.c -o /tmp/tmp4am0tehq/test.o
2025-03-24 08:59:02,300 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp4am0tehq/test.o -laio -o /tmp/tmp4am0tehq/a.out
2025-03-24 08:59:02,330 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 08:59:02,376 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmphf47uh4s/test.c -o /tmp/tmphf47uh4s/test.o
2025-03-24 08:59:02,392 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmphf47uh4s/test.o -laio -o /tmp/tmphf47uh4s/a.out
2025-03-24 08:59:02,480 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 08:59:02,480 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_08-59-02_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 08:59:02,481 - INFO - Rank:0 - Starting training pipeline
2025-03-24 08:59:02,481 - INFO - Rank:0 - Output directory prepared: save_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 08:59:02,578 - INFO - Rank:1 - Starting training pipeline
2025-03-24 08:59:02,815 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:59:02,815 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:59:03,463 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 08:59:03,463 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:59:10,953 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 08:59:11,996 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 08:59:11,996 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-24 08:59:12,037 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 08:59:12,037 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-24 08:59:12,051 - INFO - Rank:1 - Loading dataset
2025-03-24 08:59:12,051 - INFO - Rank:0 - Loading dataset
2025-03-24 08:59:12,051 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 08:59:12,051 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 08:59:12,056 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 08:59:12,058 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 08:59:12,098 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 08:59:12,098 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 08:59:12,098 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 08:59:12,098 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 08:59:13,182 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-24 08:59:13,214 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-24 08:59:13,855 - INFO - Rank:1 - Initializing trainer
2025-03-24 08:59:13,892 - INFO - Rank:0 - Trainer config created
2025-03-24 08:59:13,893 - INFO - Rank:0 - Initializing trainer
2025-03-24 09:01:26,179 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 09:01:26,228 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpitla_1j1/test.c -o /tmp/tmpitla_1j1/test.o
2025-03-24 09:01:26,247 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpitla_1j1/test.o -laio -o /tmp/tmpitla_1j1/a.out
2025-03-24 09:01:26,282 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 09:01:26,333 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpnsxc3nwg/test.c -o /tmp/tmpnsxc3nwg/test.o
2025-03-24 09:01:26,350 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpnsxc3nwg/test.o -laio -o /tmp/tmpnsxc3nwg/a.out
2025-03-24 09:01:26,422 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 09:01:26,422 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_09-01-26_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 09:01:26,423 - INFO - Rank:0 - Starting training pipeline
2025-03-24 09:01:26,423 - INFO - Rank:0 - Output directory prepared: save_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 09:01:26,535 - INFO - Rank:1 - Starting training pipeline
2025-03-24 09:01:26,770 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:01:26,770 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:01:27,389 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 09:01:27,389 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:01:27,400 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:01:28,571 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 09:01:28,571 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-24 09:01:28,584 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 09:01:28,584 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-24 09:01:28,598 - INFO - Rank:0 - Loading dataset
2025-03-24 09:01:28,598 - INFO - Rank:1 - Loading dataset
2025-03-24 09:01:28,598 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 09:01:28,598 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 09:01:28,602 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 09:01:28,605 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 09:01:28,645 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 09:01:28,645 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 09:01:28,645 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 09:01:28,645 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 09:01:29,727 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-24 09:01:29,729 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-24 09:01:30,394 - INFO - Rank:0 - Trainer config created
2025-03-24 09:01:30,394 - INFO - Rank:0 - Initializing trainer
2025-03-24 09:01:30,402 - INFO - Rank:1 - Initializing trainer
2025-03-24 09:04:07,653 - ERROR - Rank:0 - Training failed: invalid literal for int() with base 10: 'auto'
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 292, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 243, in train
    trainer = CPOTrainer(
              ^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/trl/trainer/cpo_trainer.py", line 347, in __init__
    super().__init__(
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 461, in __init__
    self.create_accelerator_and_postprocess()
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 5099, in create_accelerator_and_postprocess
    self.accelerator = Accelerator(**args)
                       ^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 516, in __init__
    gradient_accumulation_steps = int(
                                  ^^^^
ValueError: invalid literal for int() with base 10: 'auto'
2025-03-24 09:04:32,765 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 09:04:32,813 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmptvh8rcw6/test.c -o /tmp/tmptvh8rcw6/test.o
2025-03-24 09:04:32,836 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmptvh8rcw6/test.o -laio -o /tmp/tmptvh8rcw6/a.out
2025-03-24 09:04:32,932 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 09:04:32,986 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp4ox15plq/test.c -o /tmp/tmp4ox15plq/test.o
2025-03-24 09:04:33,009 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp4ox15plq/test.o -laio -o /tmp/tmp4ox15plq/a.out
2025-03-24 09:04:33,011 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 09:04:33,011 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_09-04-32_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 09:04:33,012 - INFO - Rank:0 - Starting training pipeline
2025-03-24 09:04:33,012 - INFO - Rank:0 - Output directory prepared: save_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 09:04:33,188 - INFO - Rank:1 - Starting training pipeline
2025-03-24 09:04:33,399 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:04:33,399 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:04:34,003 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 09:04:34,003 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:04:34,924 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:04:35,963 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 09:04:35,963 - INFO - Rank:1 - Resizing token embeddings from 0 to 151646
2025-03-24 09:04:36,146 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 09:04:36,147 - INFO - Rank:0 - Resizing token embeddings from 0 to 151648
2025-03-24 09:04:36,160 - INFO - Rank:1 - Loading dataset
2025-03-24 09:04:36,160 - INFO - Rank:0 - Loading dataset
2025-03-24 09:04:36,160 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 09:04:36,160 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 09:04:36,165 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 09:04:36,165 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 09:04:36,207 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 09:04:36,207 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 09:04:36,207 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 09:04:36,207 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 09:04:37,282 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-24 09:04:37,293 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-24 09:04:37,956 - INFO - Rank:0 - Trainer config created
2025-03-24 09:04:37,956 - INFO - Rank:0 - Initializing trainer
2025-03-24 09:04:37,972 - INFO - Rank:1 - Initializing trainer
2025-03-24 09:07:12,088 - INFO - Rank:0 - Resuming from checkpoint save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-24 09:07:12,088 - INFO - Rank:0 - Starting training loop
2025-03-24 09:07:12,258 - ERROR - Rank:0 - Training failed: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
- ds gradient_accumulation_steps=16 vs hf gradient_accumulation_steps=1
The easiest method is to set these DeepSpeed config values to 'auto'.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 292, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 262, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2318, in _inner_training_loop
    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 398, in deepspeed_init
    hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/integrations/deepspeed.py", line 270, in trainer_config_finalize
    raise ValueError(
ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
- ds gradient_accumulation_steps=16 vs hf gradient_accumulation_steps=1
The easiest method is to set these DeepSpeed config values to 'auto'.
2025-03-24 09:09:34,785 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 09:09:34,841 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 09:09:34,842 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=save_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_09-09-34_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=save_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 09:09:34,844 - INFO - Rank:0 - Starting training pipeline
2025-03-24 09:09:34,844 - INFO - Rank:0 - Output directory prepared: save_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 09:09:34,971 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 09:09:35,006 - INFO - Rank:1 - Starting training pipeline
2025-03-24 09:09:35,219 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:09:35,219 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:09:35,843 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:09:35,856 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 09:09:35,856 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:09:36,585 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 09:09:36,585 - INFO - Rank:0 - Resizing token embeddings from 151936 to 151648
2025-03-24 09:09:36,587 - INFO - Rank:0 - Loading dataset
2025-03-24 09:09:36,587 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 09:09:36,593 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 09:09:36,630 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 09:09:36,630 - INFO - Rank:1 - Resizing token embeddings from 151936 to 151646
2025-03-24 09:09:36,632 - INFO - Rank:1 - Loading dataset
2025-03-24 09:09:36,632 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 09:09:36,642 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 09:09:36,642 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 09:09:36,642 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 09:09:36,642 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 09:09:36,642 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 09:09:37,732 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-24 09:09:37,769 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-24 09:09:38,403 - INFO - Rank:1 - Initializing trainer
2025-03-24 09:09:38,443 - INFO - Rank:0 - Trainer config created
2025-03-24 09:09:38,444 - INFO - Rank:0 - Initializing trainer
2025-03-24 09:12:14,702 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpy3s_125e/test.c -o /tmp/tmpy3s_125e/test.o
2025-03-24 09:12:14,723 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpy3s_125e/test.o -laio -o /tmp/tmpy3s_125e/a.out
2025-03-24 09:12:14,859 - INFO - Rank:0 - Resuming from checkpoint save_models/GCR-Qwen2-0.5B-Instruct/checkpoint-1509
2025-03-24 09:12:14,859 - INFO - Rank:0 - Starting training loop
2025-03-24 09:14:49,049 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmpd6fts6iv/test.c -o /tmp/tmpd6fts6iv/test.o
2025-03-24 09:14:49,069 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmpd6fts6iv/test.o -laio -o /tmp/tmpd6fts6iv/a.out
2025-03-24 09:14:49,207 - INFO - Rank:1 - Starting training loop
2025-03-24 09:14:49,385 - ERROR - Rank:1 - Training failed: [1]: params[0] in this process with sizes [151646, 896] appears not to match sizes of the same param in process 0.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 292, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 262, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1400, in prepare
    result = tuple(
             ^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1401, in <genexpr>
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1274, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1542, in prepare_model
    model = torch.nn.parallel.DistributedDataParallel(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/utils.py", line 294, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [1]: params[0] in this process with sizes [151646, 896] appears not to match sizes of the same param in process 0.
2025-03-24 09:15:19,473 - INFO - Rank:0 - Distributed training initialized with 2 GPUs
2025-03-24 09:15:19,537 - INFO - Rank:0 - Script Arguments: ScriptArguments(data_path_list=['data/processed/rmanluo/RoG-webqsp_train'], model_name_or_path='Qwen/Qwen2-0.5B-Instruct', use_peft=False, save_merged=False, lora_alpha=16, lora_dropout=0.05, lora_r=8, max_neg_paths=3, load_in_4bit=False, load_in_8bit=False, attn_implementation='flash_attention_2', beta=0.1, loss_type='simpo', cpo_alpha=0.1)
2025-03-24 09:15:19,537 - INFO - Rank:0 - Training Arguments: TrainingConfig(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=debug_models/GCR-Qwen2-0.5B-Instruct/runs/Mar24_09-15-19_dell-Precision-5860-Tower,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_seq_length=2048,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=5.0,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=debug_models/GCR-Qwen2-0.5B-Instruct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=GCR-Qwen2-0.5B-Instruct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500.0,
save_strategy=SaveStrategy.NO,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
2025-03-24 09:15:19,538 - INFO - Rank:0 - Starting training pipeline
2025-03-24 09:15:19,538 - INFO - Rank:0 - Output directory prepared: debug_models/GCR-Qwen2-0.5B-Instruct
2025-03-24 09:15:19,621 - INFO - Rank:1 - Distributed training initialized with 2 GPUs
2025-03-24 09:15:19,660 - INFO - Rank:1 - Starting training pipeline
2025-03-24 09:15:19,882 - INFO - Rank:1 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:15:19,882 - INFO - Rank:0 - Initializing tokenizer from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:15:20,502 - INFO - Rank:1 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:15:20,509 - INFO - Rank:0 - Added 2 special tokens: {'additional_special_tokens': ['<PATH>', '</PATH>']}
2025-03-24 09:15:20,509 - INFO - Rank:0 - Initializing model from Qwen/Qwen2-0.5B-Instruct
2025-03-24 09:15:21,225 - INFO - Rank:1 - Model moved to cuda:1
2025-03-24 09:15:21,225 - INFO - Rank:1 - Resizing token embeddings from 151936 to 151646
2025-03-24 09:15:21,227 - INFO - Rank:1 - Loading dataset
2025-03-24 09:15:21,227 - INFO - Rank:1 - Loading datasets from 1 sources
2025-03-24 09:15:21,232 - INFO - Rank:1 - Combined dataset contains 6373 examples
2025-03-24 09:15:21,247 - INFO - Rank:0 - Model moved to cuda:0
2025-03-24 09:15:21,247 - INFO - Rank:0 - Resizing token embeddings from 151936 to 151648
2025-03-24 09:15:21,249 - INFO - Rank:0 - Loading dataset
2025-03-24 09:15:21,249 - INFO - Rank:0 - Loading datasets from 1 sources
2025-03-24 09:15:21,257 - INFO - Rank:0 - Combined dataset contains 6373 examples
2025-03-24 09:15:21,257 - INFO - Rank:0 - Preparing CPO dataset
2025-03-24 09:15:21,257 - INFO - Rank:1 - Preparing CPO dataset
2025-03-24 09:15:21,257 - INFO - Rank:0 - Preparing CPO dataset from 6373 examples
2025-03-24 09:15:21,257 - INFO - Rank:1 - Preparing CPO dataset from 6373 examples
2025-03-24 09:15:22,350 - INFO - Rank:1 - Created 42868 CPO samples
2025-03-24 09:15:22,371 - INFO - Rank:0 - Created 42868 CPO samples
2025-03-24 09:15:23,023 - INFO - Rank:1 - Initializing trainer
2025-03-24 09:15:23,049 - INFO - Rank:0 - Trainer config created
2025-03-24 09:15:23,049 - INFO - Rank:0 - Initializing trainer
2025-03-24 09:17:58,609 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmplyjkknk0/test.c -o /tmp/tmplyjkknk0/test.o
2025-03-24 09:17:58,631 - INFO - Rank:0 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmplyjkknk0/test.o -laio -o /tmp/tmplyjkknk0/a.out
2025-03-24 09:17:58,772 - INFO - Rank:0 - Starting training loop
2025-03-24 09:20:33,764 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -O2 -isystem /anaconda3/envs/GCR/include -fPIC -c /tmp/tmp7g0rian7/test.c -o /tmp/tmp7g0rian7/test.o
2025-03-24 09:20:33,786 - INFO - Rank:1 - gcc -pthread -B /anaconda3/envs/GCR/compiler_compat /tmp/tmp7g0rian7/test.o -laio -o /tmp/tmp7g0rian7/a.out
2025-03-24 09:20:33,928 - INFO - Rank:1 - Starting training loop
2025-03-24 09:20:34,112 - ERROR - Rank:1 - Training failed: [1]: params[0] in this process with sizes [151646, 896] appears not to match sizes of the same param in process 0.
Traceback (most recent call last):
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 292, in main
    train(script_args, training_args, rank, world_size)
  File "/mnt/wangjingxiong/think_on_graph/workflow/finetune_kg_specialized_llm_cpo.py", line 262, in train
    trainer.train(resume_from_checkpoint=checkpoint)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/transformers/trainer.py", line 2365, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1400, in prepare
    result = tuple(
             ^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1401, in <genexpr>
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1274, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/accelerate/accelerator.py", line 1542, in prepare_model
    model = torch.nn.parallel.DistributedDataParallel(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/anaconda3/envs/GCR/lib/python3.12/site-packages/torch/distributed/utils.py", line 294, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [1]: params[0] in this process with sizes [151646, 896] appears not to match sizes of the same param in process 0.
